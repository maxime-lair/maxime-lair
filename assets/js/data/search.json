[ { "title": "Bash scripting", "url": "/maxime-lair/posts/bash_scripting/", "categories": "RoadTo, Terminal-fu", "tags": "linux, bash", "date": "2022-02-03 19:00:00 +0100", "snippet": "Under construction Credits" }, { "title": "Networking concepts", "url": "/maxime-lair/posts/networking_concepts/", "categories": "RoadTo, OS Concepts", "tags": "linux, network, linklayer, ip, mac, ethernet", "date": "2022-01-11 19:00:00 +0100", "snippet": "In this article, we will try to focus on the layer 2 and 3 of the OSI model.We will try to understand IP v4/v6 works without diving too much into details, and understand how It is routed across networks.We could also call it understanding the network layer, as we will cover ARP/ICMP/MAC/LLCWe will however not dive into TCP/UDP specifics, as It would make the article too clustered with informations, and It was already partly covered in the sockets articleDatalink layerThis layer transfers data between nodes on a network segment (often Ethernet) across the physical layer (the electronic circuit). It may also detect and possibly correct errors that can occur in the physical layer. It is concerned with the local delivery of frames.A frame do not cross the boundaries of a local area network. Inter-network routing and global addressing are higher-layer functions. You could compare the datalink layer to a roundabout, It arbitrates parties for access to a medium without concern for their ultimate destination.Frame collisions occur when devices attempt to simultaneously use the same medium (e.g. a traffic crash). Data-links protocols try to detect, reduce, prevent and recover from such collisions.The main protocols are: Ethernet Point-to-Point (PPP) HDLC ADCCPEthernet 802.xxIEEE 802Here we will talk about the IEEE 802 local lan networks, with MAC layers such as Ethernet, Token Ring and 802.11 (Wireless Lan).The IEEE 802 is restricted to computer networks carrying variable-size packets, unlike cell relay networks (e.g. ADSL). It is a group of standards for LANs, numbered 802.1 to 802.12, with the most important ones being: Number Description 802.3 Ethernet 802.8 Fiber optic 802.11 WLAN Note: the number 802 has no significance: it was simply the next number in the sequence that the IEEE used for standards projects.Over the years, a few numbers were added for specific case, such as 802.15 for Wireless Personal Area Network (WPAN) and 802.15.6 for Body Area Network (BAN) used for peacemaker or body implants devices.EthernetEthernet is a family of wired computer networking technologies, and has largely replaced competing wired LAN technologies such as Token Ring or FDDI. It can go up to 400 Gbit/s and is currently developping a 1.6 Tbit/s rate as of 2021.Systems communicating over Ethernet divide a stream of data into shorter pieces called frames. Each frame contains source and destination addresses and error-checking data. This address is defined as the MAC address and is used by other IEEE 802 networking standards such as 802.8 (FDDI) or 802.11 (Wi-Fi).It is the one of the foundation that make up the Internet, as the IP is commonly carried over Ethernet. Most Ethernet devices use twisted-pair cables for the physical layer, as opposed to coaxial cable. While coaxial support greater cable lengths (200+ meters) compared to twisted pair (100 meters maximum), twisted pair is less expensive and allows speed up to 100 Gbps (Depends on cable category, 5 being standard, 8 used in datacentre).The ethernet frame:One notable frame structure of Ethernet frame is having no time-to-live field, which can lead to a switching loop (also known as broadcast storms). This can happen If there is more than one layer 2 path between two endpoints (connecting two ports on the same switch to each other), multicasts are then repeatedly forwarded forever, flooding the entire network. We will see how to avoid this in a later section.While Ethernet frames are usually 1518 bytes long, they can potentially appear smaller or bigger depending on the system settings. A larger ethernet frame is called Jumbo frames and can go up to 9000 bytes long (or you can go further and beyond with Super jumbo frames going up to 64000 bytes). If this size is non-uniform to a network node, It will be detected as Jabber and subsequently dropped. On the opposite, an Ethernet frame has a minimum size of 64 bytes (18 bytes header and 46 bytes payload), anything smaller is considered Runt frames and is subsequently dropped.You can probably notice there is multiple types of Ethernet frames, namely the one respecting IEEE 802.3 and Ethernet II. The main difference is IEEE 802.3 uses a LLC header that will be described later. Ethernet II type is largely more used in comparaison.Wi-Fi 802.11While we will not dive too deep into the 802.11 specifications, It is nice to compare it to Ethernet (wired) frames. They have different types of frames (control, management and data), we will just focus on the data frames. They are larger in size, but requires the use of a LLC header in the payload. It always starts with a frame control which contains the options of the wireless connection:It uses MAC address, like Ethernet, but differ in their header definition:Notice how similar It is to Ethernet, but allows for a bigger payload, since wireless devices are often battery powered, and they try to reduce the number of frames sent as It makes devices last longer.AlternativesIn the data-link layer, while the IEEE 802 is often used, a few alternatives exist to perform the same function. It often depends on the network hardware used to transmit data on the physical layer.Point-to-pointPPP is a data link layer communication protocol between two routers directly without any host or any other networking in between. It was designed to work with numerous network layer protocols such as IP or IPX. PPP is used over many types of physical networks such as phone line, ISDN. Your Internet Service Provider (ISP) could have used PPP to establish a connection through the facilities of the public switched telephone network.It can provide authentication through password or challenge handshake, compression and error detection. It is often used whenever you need to tunnel data over IP networks, as a tunnel is by definition a point-to-point connection and PPP is thus a natural choice between the virtual network interfaces. PPP can assign IP addresses to these interfaces that can be used to route between the networks on both sides of the tunnel. These interfaces would be called tun0 or ppp0.In case of VPN (e.g. IPSec in tunneling mode), no virtual network interfaces are created, since the tunnel is handled by the transport layer (TCP/IP), and L2TP is then used, but here PPP also provides IP addresses to the extremities of the tunnel.ADCCP / HDLCPreviously used data link layer protocol which was bit oriented. they are both functionally equivalent, and most currently used protocols in the datalink layer were derived from their specifications. It was progressively less used due to Ethernet popularity.LLC/MACThe data link layer is often divided into two sublayers: Logical link control (LLC) Media access control (MAC)LLCThe uppermost sublayer multiplexes protocols running at the top of the data link layer, It makes it possible for several network protocols to coexist within a multipoint network and to be transported over the same network medium. It also provides addressing and control of the data link. It can be considered as the interface between the network layer and the MAC. In short, the LLC provides a way for the upper layers to deal with any type of MAC layer.It can also optionally provide flow control and error management capabilities, but It depends on the protocol stack, as It is usually taken care of by a transport layer such as TCP.Note the control part can be either 8 or 16 bits long depending on the format (mostly 8 bits). We will not dive into the specifics, as It is rarely a source of bug, as you can see It is mainly to indicate the service access port.This unit is then followed by a multiple of 8 bits, containing the information of the upper layer data.But this model is rarely used in reality, and TCP/ARP frames will not use the SAP value for TCP/ARP, but will use SNAP instead. SNAP is an extension of the LLC, by adding 40 bits after the LLC header. SNAP supports identifying protocols by Ethernet type field values (Ethertypes); it also supports vendor-private protocol identifier spaces instead of being limited to the 7-bit identifying code.If the OUI value is zero, the protocol ID is the registered EtherTypeAn EtherType field in each frame is used by the operating system on the receiving station to select the appropriate protocol module (e.g., an Internet Protocol version such as IPv4). Ethernet frames are said to be self-identifying, because of the EtherType field. Self-identifying frames make it possible to intermix multiple protocols on the same physical network and allow a single computer to use multiple protocols together.This is why, on Ethernet 802.3, the 8 octets (3 from LLC, 5 from SNAP) reduce the size of the available payload such as IP to 1492 bytes (from the default MTU 1500). Therefore, with protocols that have EtherType values, packets are usually transmitted with Ethernet II headers rather than with LLC and SNAP headers, but on other network types, the LLC and SNAP headers are required in order to multiplex different protocols on the link layer, as the MAC layer doesn’t possess an EtherType field, so there’s no alternative framing that would have a larger available payload.For example, IP datagrams and ARP datagrams are transmitted over IEEE 802 networks using LLC and SNAP headers. But in reality, this specification is rarely followed, and most Ethernet frame will use Ethernet II, which include the Ethertype directly into the header, without any need for extension.Ethernet II framing vs Ethernet 802.3 framing:You can see how using Ethernet 802.3 frames reduce the payload size by the LLC+SNAP header, and overcomplicate things. This is why Ethernet II is here to stay, as It is less complicated and allow for a bigger payload. This also explains why Etherfield values are always over 1500 bytes in value. That value was chosen because the maximum length of the payload field of an Ethernet 802.3 frame is 1500 octets (0x05DC). Thus if the field’s value is greater than or equal to 1536, the frame must be an Ethernet II frame, with that field being a type field. If it’s less than or equal to 1500, it must be an IEEE 802.3 frame, with that field being a length field. Values between 1500 and 1536, exclusive, are undefined. This convention allows the coexistence of both standards on the same physical medium.MAC layerAlso called the medium access control sublayer, It controls the hardware responsible for interaction with the transmission medium (wired, optical or wireless). While the LLC provides flow control and multiplexing for the logical link (EtherType, etc.), the MAC provides flow control and multiplexing for the transmission medium.MAC access methodsIn Ethernet, bit errors are very rare in wired networks, receiving incorrect packets will simply be detected and dropped, but not retransmitted (It will expect higher layer to do it). The collision detection is these case is handled by CSMA/CD, CD meaning Collision detection.In Wireless communications, bit errors are very common, but the flow control and error management is handled by the MAC layer through CSMA/CA, CA meaning Collision Avoidance.MAC addressingIt includes a local network address called MAC address, intended to be a unique serial number assigned by the network interface hardware (NIC) at the time of manufacture. It is 48 bits long, separated by colons every two digitsAn example, this one belongs to Samsung Electronics:a MAC address can be defined universally by the manufacturer or locally by a system administrator. MAC address are by definition finite and we will end up running out of possible addresses, so an alternative called EUI was created. It simply adds 2 octets to the UAA, all MAC-48 are EUI by padding FF:FF between your OUI and UAA.Ethernet frames with a value of 1 in the least-significant bit of the first octet of the destination MAC address are treated as multicast frames and are flooded to all points on the network.Some blocks are reserved to specific usage, such as PTP (Time precision protocol) or STP (Spanning tree protocol), you can check them here. These blocks are either only on Local LAN link or either be forwarded through bridges.Easier to understand with an example:The MAC protocol is used to provide the data link layer of the communication protocol (e.g. Ethernet), and its header include 16 bytes with a CRC at the end:Quality of service controlAlso called Audio Video Bridging (AVB), It provides a set of technical standard to improve synchronization, low-latency and reliability for switched Ethernet networks. It is particulary useful in QoS applications requiring low jitter such as voIP and IPTV. This QoS is often implemented on the network/transport layer, but there is a few enhancements available on the datalink layer to implement it. These enhancements make use of the optional VLAN tag to implement priority values on data frames.VLANsA virtual LAN is any broadcast domain that is partitioned and isolated in a network at the data link layer. It works by applying tags to network frames to create the appearance and functionality of network traffic that is physically on a single network but acts as If It is split between separate networks. VLANs allow admins to group hosts together even if the hosts are not directly connected to the same network switch, greatly simplifying network design and deployment (less cables/devices).An ethernet frame with a VLAN value is usually defined as tagged, and without the optional VLAN header, It is untagged. An Ethernet switch can decide to drop or forward tagged frames depending on its configuration (access or trunk mode, VLANs allowed, etc.).This feature is one of the most used in modern networks, as you can scale multiple virtual switches inside one physical switch. Admins will use it to separate and isolate services, making the subnet size smaller (less broadcast noises) and more secure (easier to apply IPS/IDS/ACLs). Cloud providers are particulary fond of VLAN, as you can more easily scale your growing infrastructure.Switch/Bridge/WAPSpeaking of VLANs direct us to the subject of Ethernet switch and bridges, and how they operate on the datalink layer.Let’s start off with a hub, which is a network device without any logic implemented, It simply waits to receive incoming packets to blast it out on all other ports. It will cause network congestion due to the increased overhead and lots of Ethernet collisions (e.g. 6 hosts sending one packet each to a hub would result in creating 30 packets total on the network). This helps to understand why network devices need to implement some kind of logic to route packets efficiently.The first network device to implement this logic on the datalink layer is the Ethernet switch. It uses the MAC destination address to forward a frame to the port associated with that address. Addresses are automatically learned by looking at the MAC source address on received frames. If a MAC address is unknown, It will simply flood the frame out to all its port but the ingress port, which will simply refresh its MAC address table.A switch is stateless, so It has no memory on who requested which data, It does not operate on any upper layer protocols (IP, TCP, HTTP, etc.), It simply learn source addresses and forward by destination address. If you need to capture frames going from one port to another on a switch, you would use SPAN (i.e. port mirroring). However, this setup is limited and impacts your switch CPU. In reality people would prefer adding a network TAP (Terminal Access Point), which is a hardware device similar to a hub, but with 2 ports (port A and B) and a monitor port. They are non-obtrusive (they do not impact your ethernet switch), not detectable on the network, but usually come at a cost.Now, if you understood what an ethernet switch is, you already know what an ethernet bridge is. The difference is simple, an ethernet switch is a multiport ethernet bridge. It simply relays Ethernet frames between devices connected to different ports, in bridge case, It would only be between two devices. Today, the term bridge and switch pretty much mean the same thing.Historically, switches were a term used for devices working on the datalink layer, but It merged overtime with the term router, usually used for the network layer. Router includes the logic of the Internet Protocol which will be explained later, but this made switches capable of performing ARP requests to update its MAC address table. While switches were already sensitive to MAC flooding attacks, this also made them vulnerable to ARP spoofing.As your network size grows, the number of switches (physical or virtual) goes up, up to thousands of bridges in some case. In order to provide the best route between two nodes, two features to be added onto an Ethernet switch: Redundancy (through STP) Load-balancing (through SPB)LANs have traditionally relied on Spanning Tree Protocol (STP) and its variants (RSTP/MSTP) to prevent loop on the L2 layer. This topology is achieved by electring a root bridge and building a least-cost tree linking the root bridge with other non-root nodes. This least-cost tree is created by disabling all links which are not in the least-cost path towards the root. By being able to create a tree-like topology, we can also add more switches for redundancy without being threatened by a layer-2 loop.While It prevents loop by forcing the path through the root bridge, It makes many links to remain unused, forcing sub-optimal paths. It showcases the limits of Ethernet networks, where all nodes in the LAN have to learn all end-device MAC addresses by flooding until the destination address is learned.For example, imagine this topology:The forced path would be the least-cost ones going through the root bridge:But It is not using all possibles paths, specially a potentially more efficient one:SPB stands for Shortest Path Bridging, replacing older spanning tree protocols that blocked any redundant paths that could result in a broadcast storm. SPB allows all paths to be active with multiple equal cost paths, providing much larger layer 2 topologies, improving the efficiency by allowing traffic to load share across all paths of a mesh network. It is designed to virtually eliminate human error during configuration (which still happens today) and preserves the plug-and-play nature that established Ethernet as the de facto protocol at layer 2. It started being used since ~ 2014 (e.g. at the Winter Olympics).A nice gif from wikipedia showcasing the algorithm:Network layerNow that we have understood how the data link layer is applied, we can move on onto the network layer. It is responsible for packet forwarding including routing through intermediate routers.It is connectionless, a data packet can travel from a sender to a recipient without the need for an acknowledgement. Also, every host on the network must have a unique address that determines where it is. Networks also need to be able to forward message to other networks for wide-area communication, with the help of gateways or routers.The main protocol of this layer is Internet Protocol or IP. It has the task of delivering packets from the source host to the destination host solely on the IP addresses in the packet headers. It is by definition an unreliable protocol, as It only ensures delivery on a best-effort basis, It does not guarantee that data will be delivered.IPv4Similar to the logic in other OSI layers, an IP packet consists of a header section followed by the data It encapsulates. One difference could be that It does not add a footer, so there is no data checksum for the packet, It is usually fine as the data link layer, through the CRC of Ethernet or FCS of 802.11 WLAN, can already detects most errors, and higher layers also adds data checksum (e.g. TCP for the transport layer).An IPv4 header format looks like this:You can easily deduce from the different section that IPv4 is mainly used for encapsulating the higher layer, while allowing fragmentation and routing.IPv4 addressThe IP address space is 32-bit, allowing for 4 billions unique addresses, It is managed by the IANA and the regional Internet Registries (RIP NCC for Europe, ARIN for America). Each RIR maintains a publicly searchable whois database that provides information about IP address assignments.For example, on a random European IP address:You can notice the IP is part of an address range, as those registries do not allocate IP one by one but block by block. Each block can be defined with a CIDR notation, which combines the address of its routing prefix in a compact format, in which the address is followed by a slash character (/) and the subnet mask.Let’s say you wish to use 16 IP addresses in your network, you need to know two parts : your network identifier and the host identifier. This is done through a mask.Here, the network identifier would be 192.168.31.128 (chosen randomly), and the IP block 192.168.31.128 - 192.168.31.143 ; notice how there is only 14 available address in the range, as the first one is network address, and the last one is the broadcast address. In CIDR notation, the mask would be /28 as there are 28 binary values that identify the network identifier. In reality, there would only be 13 available addresses, as you would need to count the gateway to route outside your network.A few address block are reserved for special use, notably 127.0.0.0/8 for loopback addresses, 10.0.0.0/8 for local communication in VPNs or 224.0.0.0/4 for IP multicast. There is also three private networks block available, packets addresses in these ranges are not routable in the public Internet. These private hosts are notably used for desktop systems : CIDR block Address range Number of addresses 10.0.0.0/8 10.0.0.0 - 10.255.255.255 16 777 216 172.16.0.0/12 172.16.0.0 - 172.31.255.255 1 048 576 192.168.0.0/16 192.168.0.0 - 192.168.255.255 65 536 Fragmentation and reassemblyThe IP enables traffic between networks who are usually of diverse physical nature (no one has the same hardware), harnessing different transmission speed and MTU. When one network wants to transmit datagrams to a network with a smaller PDU, It may need to fragment its datagrams. This is done on the network layer by IPv4 routers.This is only possible if a packet allows itself to be fragmented, as seen on the IPv4 flags header. At each fragmentation, 4 elements will require to be changed : length, flag, fragment offset and header checksum. One interesting feature is the possibility of fragmented packet to be re-fragmented thanks to the fragment offset being a 8-byte blocks multiple.For example, imagine sending a 3320 bytes packet through this topology with different MTUs:While this depend on some configuration, most routers will not re-assemble even if the sum of two fragments are under the MTU, and It will only be done once the packet arrives onto its destination.ARPNow that we understand how IPv4 addressing and fragmentation works, we can try to understand how IPv4 address are linked to their lower layer address such as MAC. This specific resolution is done through ARP which is address resolution protocol. Since this protocol resolves layer 3 addresses with layer 2 addresses, It can be considered a link and network layer protocol.The protocol uses a simple message format containing one address resolution request or response. The following example would be for IPv4 over Ethernet ARP packet, since hardware type (MAC) is 48 bits and protocol address (IPv4) is 16 bits. All systems retain an ARP cache table to avoid re-doing this request everytime.ARP can also be used as an announcement protocol where a host can periodically broadcast a gratuitous ARP (GARP) message to announce its IP or MAC address change. It is usually done at startup, but will avoid flooding the network with those requests, as It could be considered ARP spoofing by malicious hosts.IPv6IPv6 is the most recent version of the Internet protocol. It was developped to deal with the long-anticipated issue of IPv4 address exhaustion. 4 billions devices is not a lot considering the world population and growing appetite for electronic devices such as smartphones. Today, IPv6 deployment is largely popular in higher population countries such as China or India, but most countries still largely use IPv4 in comparaison. It is deemed to be changed over the next few decades, as China announced plans to complete a national IPv6 rollout by 2030.In IPv6, devices are assigned a unique IP address for identification and location definition. IPv6 uses 128-bit addresses, allowing around the same number of atoms available on Earth (e.g. pratically infinite).IPv6 has a minimum packet size of 1280 bytes, consisting of 40-byte base header and 1240 bytes of payload.IPv6 does not fragment its packets, as It uses Path MTU Discovery protocol to determine the network path between two hosts, this protocol is simply about activating the DF flag in IP header and record its path.A few address types exist : Address type Sub-type Description Example Unicast Global Unique address Globally reachable by any host 2001:581:f3d1:241f::/64 Unicast Link-local Required on every interface, but packets cannot leave or enter the interface fe80::/10 Unicast Loopback Same as 127.0.0.1/8 in IPv4 ::1/128 Unicast Unique local addresses Equivalent to IPv4 private ranges fc00::/7 Multicast   Used to send a packet from one to many (always start with ff) ff00::/8 Anycast   Used to send a packet from one to a group but only send to the first least expensive destination   NDPIt shares the same function as ARP in IPv4. It uses five ICMPv6 packet types for the purpose of : Router solicitation Router advertisement Neighbor solicitation Neighbor advertisement RedirectI will not describe the details, as we just to know It reproduces ARP function for MAC address resolution on IPv6.ICMPIt is used by network devices to send error messages and operational information indicating success or failure when communicating with another IP address. It is always encapsulated in an IPv4 packet, so It is part of the transport layer.For example, every device forwarding an IP datagram first decrements the TTL field in the IP header by one. If the resulting TTL is 0, the packet is discarded and an ICMP time exceeded in transit message is sent to the datagram’s source address. This is what the command traceroute essentially does.A header format is defined as such:ICMP error messages contain a data section that includes a copy of the entire IPv4 header, plus the first eight bytes of data from the IPv4 packet that caused the error message. It can go up to 576 bytes in length.A few examples of control messages types: Type value Type Subtype (code) Description 0 Echo reply 0 Used to ping 3 Destination unreachable 0 to 15 Indicates the routing reason 4 Redirect message 0 to 3 Used for ToS or by network &amp;amp; host 11 Time exceeded 0 to 1 Reassembly or TTL expiration 12 Bad IP header 0 to 2 Missing parameter or option, wrong computation in IPv4 header Some routers do not allow ICMP message to pass through due to security reason as It can be used to probe your network, but It is most often used for debugging and troubleshooting reason.Nat / MasqueradeNetwork address ranslation (NAT) is a method of mapping an IP address space into another by modifying network address information in the IP header of the packets while they are in transit across a traffic routing device.It has become an essential tool in conserving global address space in the face of IPv4 address exhaustion. In fact, one Internet-routable IP address of a NAT gateway can be used for an entire private network.There is two types of NAT operations: Destination-based (DNAT) Source-based (SNAT) :triangular_flag_on_post: For some vendors, SNAT can also mean stateful (e.g. Cisco) or secure (e.g. F5 Networks or Microsoft)DNATDestination network address translation is a technique for transparently changing the destination IP address of a routed packet and performing the inverse function for any replies. Any router situated between two endpoints can perform this transformation, and It is commonly used to publish a service located in a private network on a publicly accessible IP address. This use is also called port forwarding or DMZ when used on an entire server. If you wish to protect your server behind a firewall, this is a needed feature.SNATAlso called One-to-Many NAT, It is a technique used for transparently changing the source IP address of a routed packet and performing the inverse function for any replies. The majority of network address translators map multiple private hosts to one publicly exposed IP address. The way It works is a router in that network has a private address of that address space. The router is also connected to the Internet with a public address.As traffic passes from the local network to the Internet, the source address in each packet is translated on the fly from a private address to the public address. The router tracks basic data about each active connection (particularly the destination address and port). When a reply returns to the router, it uses the connection tracking data it stored during the outbound phase to determine the private address on the internal network to which to forward the reply.To logic works well for one-to-one conversion, but in order to map multiple address (one-to-many), a feature called masquerade is needed. It allows all of the hosts on a private network to use the Internet at the price of a single IP address. The word masquerade comes from a party or dance where people wear masks, and It pretty much defines the logic behind it.Using Masquerade today is a must, as It also forbid external network to access any of your private network since none of the hosts on the supported network behind the router are ever directly seen, and lays the foundation for applying security measures to your private hosts. It comes at a cost, since It requires CPU ressources for the translation and forbid you from hosting services relying on incoming sessions to work (such as FTP).OSPFTo end the presentation of the network layer, we can talk about the Open Shortest Path First (OSPF) routing protocol used for IP networks. It is widely used in large enterprise networks and large service provider as the de-facto protocol in a single routing domain. It is part of Interior gateway protocol (IGP), alternatives are RIP or IS-IS.It gathers link state information from available routers and constructs a topology map of the network. The topology is then used to routes packets based solely on their destination IP address. OSPF detects changes, such as link failures, and converges on a new loop-free routing structure within seconds. It computes the shortest-path tree for each route based on Dijkstra’s algorithm.OSPF uses multicast addressing for distributing route information within a broadcast domain to all IP routers but without traversing them.You can try it out here, for example:Useful commands and utilitiesI will be using a CentOS 9 Stream which came out in late 2021, some commands might differ depending on the Linux distribution you use, but the way It works will remain mostly the same.IP commandYour main tool out of the box will be the ip command, It is a recent command, and people often still uses ifconfig instead. ip utility regroups many tools into one single command, such as arp, route, ifconfig, netstat. If you use it in scripts, make sure the command is installed !I created some hosts in a private LAN, and have them able to access the Internet, check out the PFSense article for more detailsFirst, let’s show our network interface with ip linkWe could even print statistics on their use with the -s argument:This command will show you the linklayer interfaces, and thus only the MAC addresses for each device, they show you the main characterics, which is mainly about : Device enabled and connected (e.g. UP and LOWER_UP respectively) The device capability (BROADCAST, MULTICAST) Available MTU (notice the loopback higher MTU as It is in memory only, this value was bumped up in 2012) Some QoS values the assigned MAC value and its broadcastWe can even use it to bring our device down with ip link set &amp;lt;interface&amp;gt; up/downBut It requires sudo-access since It could potentially impact others users.Now, let’s move onto the network layer, and show our IP address with ip addressNotice how the command returns the linklayer informations, and then an IPv4 and IPv6 address. You can see the loopback address being 127.0.0.1/8 and ::1/128, but more interestingly, the ens32 device has a private IPv4 address (private block 10.x.x.x) and link-local IPv6 address (starts with fe80::).The utility also allows us to check the multicast address available with ip maddr:And our local ARP table with ip neigh, which is empty at first, but get filled as we start discussing with others hosts on LAN:The command ip route can also be used to print our routing table, used by our host to route packets on the network:Here you can see two simple rule which says: route everything by default to our gateway located on 10.0.0.1 through ens32 network interface Anything directed onto the private IP block 10.0.0.0/24 can be reached on device ens32 with IPv4 source address 10.0.0.2These rules are applied from the top and then goes down, and If It doesn’t match any rule, It simply get dropped. You can notice that since the default rule is first, the second one will never be applied.While this is not dependent on the layer 2 or 3, you can also use ss to show your host services open to the network/etcMost networks configuration are located in /etc, but the exact files are not always the same depending on your distribution.One of the most common interface file is /etc/sysconfig/network-scripts/ but here mine is empty:But I can definitely see some network interfaces defined in /etc/networksIn my distribution, those settings are done through NetworkManager and located in /etc/NetworkManager/system-connectionsYou can use nmcli to interact with it/procA lot of informations about your configuration can also be accessed directly onto /proc/netYou focus on accessing process specific statistics, such as the ARP_cache:/sysYou can also access the /sys directory for more device related informations, a simple search can help us where to look for:Which allows us to check ens32 configuration:Which was a symbolic link to our hardware device definition:ConclusionWe have seen how the linklayer uses frame to transport data across a physical medium, and how It encapsulates its data thanks to MAC addressing. This allows the network layer to use IP for fragmentation and global addressing on a wider variety of networks. While IPv4 and IPv6 can be different in the way they work, they share the same idea of providing a unique address for a device across the globe. A lot was to cover, between ethernet/wifi frames, IP addresses and its link to MAC through ARP, and how NAT try to cover the problem of IP exhaustion. A few commands allows you to handle this on your host, and they are usually vital for people to learn whenever they create a new host, as they are your gateway to the entire Internet.Not everything was explained, and if you wish to focus your attention on a specific subject, check out the links below for more information. Credits https://en.wikipedia.org/wiki/Subnetwork_Access_Protocol https://networkengineering.stackexchange.com/questions/732/introductory-level-explanation-of-vlans https://www.al-enterprise.com/-/media/assets/internet/documents/spb-architecture-tech-brief-en.pdf https://www.redhat.com/sysadmin/what-you-need-know-about-ipv6 https://www.oreilly.com/openbook/linag2/book/ch11.html https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf" }, { "title": "Sockets", "url": "/maxime-lair/posts/sockets/", "categories": "RoadTo, OS Concepts", "tags": "linux, socket, tcp, udp", "date": "2022-01-10 19:00:00 +0100", "snippet": "Here we will study the different sockets available and how they function. We will try to implement some, and check out how to monitor them.DefinitionOperating systems include the BSD interprocess communication facility known as sockets. They are communication channels that enable unrelated processes to exchange data locally and across networks. The Berkeley sockets API represents it as a file descriptor (file handle) in the Unix philosophy that provides a common interface for input and output to streams of data. Berkeley sockets evolved with little modification from a de facto standard into a component of the POSIX specification. The term POSIX sockets is essentially synonymous with Berkeley sockets, but they are also known as BSD sockets.A socket uniquely identifies the endpoint of a communication link between two application ports. A socket is typically representated through five informations: The domain The type The protocol The hostname The portThis quintuple is used to correctly identify a socket on a host. This host will add this socket to its local table by associating it with a socket descriptor (an integer).While a socket is mostly used over a network to allow two hosts to communicate between each other, communication between local processes (e.g. IPC) also makes use of socket. They separate into a few different family, or domain: AF_UNIX for UNIX domain family, also called IPC socket AF_INET for IPv4 Internet socket AF_INET6 for IPv6 Internet socketThe behaviour is similar, but in IPC socket, rather than using an underlying network protocol, all communication occurs entirely within the OS kernel. There is many more socket families existing, but they are often used for very specific use-case, such as Bluetooth, or virtual machine to host. AF stands for Address family while PF stands for Protocol family if you are wondering. AF refers to addresses from the internet (e.g. IP), while PF can refer to anything in the protocol, usually socket/port.Sockets are classified according to communication properties. Each socket has an associated type, which describes the semantics of communications using that socket. Each socket type incurs different properties such as reliability, ordering and session control. The most common ones are : Stream socket Datagram socket Raw socket Unix domain socketThen, the socket has to support a protocol to allow data exchange through a standard set of rules such as UDP or TCP over IP.I wrote a simple script in Python3 to showcase the different family/type/proto used by each socket. The choice of Python3 is arbitrary, I could have used any languages with a socket library (almost all). My point here will be to use this to create a client in Go, and a server in Python, to showcase that It is language-agnostic.The code:The output:We will go through each socket type, and implement a simple client/server at the end. Different transport layer protocols will be used and explained, for lower levels protocols, check out the networking concepts article.Socket creationSockets by themselves are not particularly useful. The purpose of a socket of course is to communicate with other sockets. In order to create a socket that is able to communicate, you must follow a few mandatory steps.The most common relationship between two sockets on remote machines is that of a server and a client. The server sockets waits in an open state (Listen) for a client to communicate with it. Or perhaps It broadcasts messages so any clients that listening will receive it.First, we need to create this socket with a specific communication profile. It usually follow the following patternAs you can see, the server should start before the client, and be in an Accept state before exchanging data. On the client side, It has to match the family/type/protocol in order to match the connection. At the end, the client or the server can decide on closing the exchange, but the server still has to stop itself afterwards (since there could have been other clients).In an Internet socket, the address would be an IP/Port, but on an IPC, the address would be local, so a file system node.Socket typesHere we will check the different type of socket type available to us. There is a few to know, mainly in order to implement TCP/UDP and IPC.Stream socketConnection-oriented sockets, which use Transmission Control Protocol (TCP), SCTP or DCCP. A stream socket provides a sequenced and unique flow of error-free data without record boundaries, with well-defined mechanisms for creating and destroying connections and reporting errors. A stream socket transmits data reliably, in order, and with out-of-band capabilities. On the Internet, stream sockets are typically implemented using TCP so that applications can run across any networks using TCP/IP protocol.This is the most used type of socket today, as It ensures what the server send will be received in the same sequence on the client. Imagine watching a movie, you would not want the image to stutter and miss frames every few seconds, so you prefer waiting for it to buffer reliably and watch it later.They are represented by the family/type: AF_INET/SOCK_STREAMDatagram socketConnectionless sockets, which use User Datagram Protocol (UDP). Each packet sent or received on a datagram socket is individually addressed and routed. Order and reliability are not guaranteed with datagram sockets, so multiple packets sent from one machine or process to another may arrive in any order or might not arrive at all. Special configuration may be required to send broadcasts on a datagram socket. In order to receive broadcast packets, a datagram socket should not be bound to a specific address, though in some implementations, broadcast packets may also be received when a datagram socket is bound to a specific address.UDP sockets do not have a established state. A UDP server process handles incoming datagrams from all remote clients sequentially through the same socket. UDP sockets are not identified by the remote address, but only by the local address.This is a less reliable type of socket, but It focuses on delivery speed. If you already have a protocol on higher OSI layer that implements session and reliability, then UDP could be considered. Whenever there is a high bandwidth requirement, or you need to implement multicast, you can consider this socket type. In case of weather data for example, you do not care if you miss out on a few seconds data, but you care to receive it as fast as possible to prepare for a storm or earthquake.They are represented by the family/type: AF_INET/SOCK_DGRAMRaw socketProvides access to internal network protocols and interfaces. This type of socket is available only to users with root-user authority. Raw sockets allow to have direct access to lower-level communication protocols. Raw sockets are intended for advanced users who want to take advantage of some protocol feature that is not directly accessible through a normal interface, or who want to build new protocols on top of existing low-level protocols.Raw sockets are normally datagram-oriented, though their exact characteristics are dependent on the interface provided by the protocol. Raw sockets are typically available in network equipment and are used for routing protocols such as IGRP and OSPF, and for Internet Control Message Protocol (ICMP).They are usually available in most family, as one type, and they have no preferred protocol.Unix domain socketAlso called Inter-process communication socket (IPC), they are a data communications endpoint for exchanging data between processes executing on the same host operating system. While they do not implement TCP/UDP/IP as they are not over the network, they still include stream/datagram sockets capabilities. They are a standard component of POSIX operating systems.Available on the AF_UNIX family, the SOCK_STREAM socket type provides pipe-like facilities, while the SOCK_DGRAM and SOCK_SEQPACKET socket types usually provide reliable message-style communications.As a more pratical example, we can use nc command to create them on the fly. As you can see, the address is simply the path on the filesystem.It allows for a two-way communication (so by default a stream socket), be careful when creating them through this command, as It doesn’t destroy the socket file afterwards.Note: Named pipes are another mean of IPC within a Unix host, but they are only allow uni-directonial communication and can not distinguish clients from each other. UNIX socket are often considered a cleaner way for process to communicate between each other.Socket protocolNow that we have seen the different socket families (Unix, Internet IPv4, IPv6..), and socket types (Stream, datagram, raw, etc.), we can dive in the last part: the socket protocol.A protocol is a standard set of rules for transferring data, such as UDP/IP and TCP/IP. The protocol has to be supported by the socket type (and by the socket family) to be used. While in most cases, the protocol will be automatically decided depending on the socket type (INET/Stream -&amp;gt; TCP ; INET/Datagram -&amp;gt; UDP), It is not always the case. Here is a few major protocols in the suite of Internet Network Protocols: TCP UDP RDS IP ICMPUDPWithout diving too far in the OSI layer, know those protocols stands on the transport layer, on top of the network that implements IP.It is connectionless, offers datagram services but less reliable. While It is less popular than TCP, It is gaining traction through big scale stateful UDP services, such as QUIC.UDP sockets can be either : connected containing Source IP / Port Destination IP / Port unconnected Bind IP / Port An example with code, to ping Cloudflare DNS server with UDP packet:And It will timeout as the DNS server can not answer the request:As you can see, connected is great if you already know where you are going, but unconnected would be more fit for a server, as one socket can make multiple outbound queries.However, UDP requires a bit more tuning when used, as TCP can transparently deal with MTU/fragmentation (e.g. with jumbo frames) and ICMP errors, while UDP might require extra care on those corner cases.The UDP packet header:TCPTCP provides reliable stream delivery of data between Internet hosts.Like UDP, TCP uses the Internet protocol (IP) to transport data, and supports the block transmission of a continuous stream of datagrams between process ports. TCP ensures that data: is not damaged (checksum) lost/duplicated (sequencing) delivered out of order (acknowledgement)The packet header looks like this:The following are operational characteristics of TCP: Item Description Basic Data transfer TCP can transfer a continuous stream of 8-bit octets in each direction between its users by packaging some number of bytes into segments . TCP implementation allows a segment size of at least 1024 bytes Reliability A sequence number is assigned to each octet it transmits and requiring a positive acknowledgment (ACK) from the receiving TCP. If the ACK is not received within the time-out interval, the data is retransmitted. The TCP retransmission time-out value is dynamically determined for each connection, based on round-trip time Flow control TCP governs the amount of data sent by returning a window with every ACK to indicate a range of acceptable sequence numbers beyond the last segment successfully received Multiplexing TCP allows many processes within a single host to use TCP communications facilities simultaneously. TCP receives a set of addresses of ports within each host Connections TCP must initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection Precedence and security The priority function is provided to allow TCP to mark certain packets as higher priority. Packets with higher priority will get forwarded first. In addition, a provision is made to allow for compression and encryption of the TCP headers. All of these functions are signalled by a set of flags in the TCP header As you can see, while the header is larger than in UDP case, It adds useful features for your connection. The way It operates is also different, as It is divided into three phases: Connection Establishment (through three-way handshake –&amp;gt; SYN + SYN/ACK + ACK) Data transfer Connection termination (through four-way handshake –&amp;gt; FIN + ACK - FIN + ACK)Now that we understand how It operates, and what It can offer, let’s talk about a few features and options in more detailsFlow controlBesides what checksum and sequence can achieve, TCP includes a flow and congestion control to avoid flooding one node. Having a mechanism for flow control is essential in an environment where machines of diverse network speeds communicate. TCP uses a sliding window flow control protocol, where each segment specifies the number of data It is willing to buffer for the connection. The sending host can send only up to that amount of data before It must wait for an acknowledgement and window update from the receiving host.Congestion controlMSS is the largest amount of data, specified in bytes, that TCP is willing to receive in a single segment. For best performance, It should be set small enough to avoid IP fragmentation so It can pass through a link MTU. This parameter is typically announced on the connection establishment and derived from the MTU size of the data link layer of the networks.The next aspect is congestion control, where TCP uses mechanisms to achieve high performance and avoid congestion collapse (when incoming traffic &amp;gt; outgoing bandwidth). It keeps the data flow below a rate that would trigger collapse through four intertwined algorithms: slow-start, congestion avoidance, fast retransmit and fast recovery. Without diving into the specifics, these algorithms set a small multiple of the MSS allowed on that connection depending on the round-trip time (RTT).There is a lot more to cover on this part, as there is new algorithms coming in and out every year, and It can also depends on the network visibility (black-box or white-box).Selective acknowledgmentsAlso called SACK, It allows the receiver to acknowledge discontinuous blocks of packets which were received correctly, in addition to the sequence number immediately following the last sequence number received. This option is not mandatory, but has become widespread due to the quality of life improvement on long fat networks.TCP may experience poor performance when multiple packets are lost from one window of data. This forces the sender to either wait a roundtrip time to find out about each lost packet, or to retransmit segments which have been correctly received. With SACK, the data receiver can inform the sender about all segments that have arrived successfully, so the sender need to only retransmit the segments that have actually been lost.TCP no delayNow we will check out a few options that are relevant for Real Time applications, the first one being TCP_NODELAY.TCP has had to introduce new heuristics to handle the changes effectively. These heuristics can result in a program becoming unstable. One example of heuristic behavior in TCP is that small buffers are delayed. This allows them to be sent as one network packet. This generally works well, but it can also create latencies.TCP_NODELAY is an option that can be used to turn this behavior off. For it to be used effectively, the application must avoid doing small buffer writes, as TCP will send these buffers as individual packets.TCP corkAnother TCP socket option that works in a similar way is TCP_CORK. When enabled, TCP will delay all packets until the application removes the cork, and allows the stored packets to be sent. This allows applications to build a packet in kernel space, which is useful when different libraries are being used to provide layer abstractions.GRPCNow that we have seen the main protocols (TCP/UDP), we can check out a different type called gRPC. It is an open source remote procedure call (RPC) system initially developed at Google in 2015. It uses HTTP/2 for transport, with protocol buffers, bidirectional streaming and flow control. Most common usage scenarios include connecting services in a microservices style architecture.It stands on the session layer, and could be considered a completly different kind from the previous sockets we saw, but they still create communication channels that enable unrelated processes to exchange data, and It is a very new way for process to communicate, so let’s try it !We will implement a simple client in Go with BubbleTea TUI, that will interact with a python server. It will simply provide a list of possible sockets family, then types, then protocols (so we can re-use our previous script). Why ? Because I saw those technologies and I thought It would be cool to use them.The code is available here: grpc-goclient-pythonserverThe output:Interact with socketNow that we know how to create and implement sockets, let’s check out how we can show and analyze them on Linux.Useful commandsLinux includes a nice array of tools to display network activity. One of the most used one is netstat but It is on its way to become obsolete, and leave his seat for the ss utility. It is supposed to be faster and more human-readable.NetstatThe main example of netstat usage would be to display sockets on LISTEN stateThe command:You can notice some well known ports, such as SSH (22), Performance monitor daemon (44321) but also more obscure one such as 4330, no idea of its use, and 50051 which was our previous GRPC project server.There is many more arguments available, but you usually only care for TCP listen connections when handling server.For example, to only show dbus unix sockets:ss utilityThe basic ss command without any options simply lists all the connections regardless of the state they are in. Luckily, It is easy to transition into ss from netstat as most options operate in much the same fashion.To check out the TCP sockets with their associated cgroupsOr Unix stream sockets:One nice command to check non-localhost sockets:Others utilitiesSince everything is a file in Linux, you can also use lsof to check which sockets are in useFor Unix sockets:To show the peer :You can also use nc to start a quick listening socket on any ports, for testing purpose, such as :MonitoringIn order to properly monitor our sockets activity, we need to define our search perimeter.Sockets are dependent on your network configuration (devices, routing, etc.) and your program ability to process them. It is necessary to have metrics from your hardware, network and process to fully analyze an issue. For example, you could think changing from TCP to UDP could increase your latency, but If you had routing in place depending on TCP or UDP packets in your network, this could skew the results.This is why we want to focus on what metrics we can retrieve from our sockets. ss -s command already gave us a nice idea of the type of information we could be looking for, but where does It take this information from ?/procOne of the first place we can look into is /proc, where we can find networking parameters and statistics.We can find some nice statistics in /proc/net, i.e. protocols :Or sockstat / tcp:Thinking of metrics, we could first start from a top-down perspective, count the number of sockets, by family, type and protocols, then get more specific statistics with popular ones (such as TCP).For example, taking TCP as a basis, we can check all stats available for TCP:We can see there is 102 possibles stats for a TCP connection, It could be interesting to check out each of them, as they could be useless or useful depending on your usecase. If you do not have SACK activated, you probably won’t need it.You can also check out the kernel networking parts in /proc/sys/net, where you can see your system networking parameters:More information on each directory is available here/sysYou can also find information in the /sys directoryBut most of these informations will depend on your devices, and be most hardware oriented.ConclusionWe were able to check out the difference in multiple socket families, types and protocols, with the most popular being TCP/IP, UDP/IP, Unix/DGRAM, Unix/STREAM. We tried to dive into the specifics of TCP and UDP protocols to learn how and why they are different. We created a small project to explore new technologies such as GRPC through a nice golang client and python server, while having a BubbleTea TUI was not important, It was still nice to implement and try it out. We ended up with some commands and metrics we can use to view our system sockets.Check out some of these credits below and the GRPC repository I linked, as I think I over-complicated some parts and not explained some others, this is one of the longest article I did, mainly due to the client/server side-project. Credits https://www.ibm.com/docs/en/zos/2.2.0?topic=functions-socket-create-socket https://www.digitalocean.com/community/tutorials/understanding-sockets https://www.ibm.com/docs/en/zos/2.2.0?topic=concepts-tcpip https://www.ibm.com/docs/pl/aix/7.1?topic=concepts-sockets https://ipfs.io/ipfs/QmfYeDhGH9bZzihBUDEQbCbTc5k5FZKURMUoUvfmc27BwL/socket/services.html https://blog.cloudflare.com/everything-you-ever-wanted-to-know-about-udp-sockets-but-were-afraid-to-ask-part-1/ https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/chap-sockets https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s2-proc-dir-net" }, { "title": "Threads and concurrency", "url": "/maxime-lair/posts/threads_and_concurrency/", "categories": "RoadTo, OS Concepts", "tags": "linux, concurrency, threads", "date": "2022-01-09 19:00:00 +0100", "snippet": "In this article we will talk about threads and writing programs with concurrency in mind. I will try to dive in the definition of threads, how they are used today, and the common pitfalls we can encounter.Note: I’m using a CentOS 9 Stream which came out last month, so a few things might differ from your distro.DefinitionA thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler. In most cases, a thread is a component of a process. The multiple threads of a given process may be executed concurrently, sharing resources such as memory while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its dynamically allocated variables and (non-thread-local) global variables at any given time.Popularity of threading has increased around 2003, as the growth of the CPU frequency was replaced with the growth of number of cores, in turn requiring concurrency to utilize multiple cores. Even today, finding programs which include multi-core support is rare.A process is a unit of resources, while a thread is a unit of scheduling and execution. There is typically two types of threads: kernel and user. Kernel thread is the base unit of kernel scheduling, each process possess at least one, and owns a stack, registers and thread-local storage. User threads are the opposite, as they are managed and scheduled in the userspace, the kernel is unaware of them, this makes them extremely efficient at context switching.Using a process is relatively expensive, as they own resources allocated by the operating system such as memory, file and devices handles or sockets. They are isolated and do not share address spaces or file resources unless you use an IPC. This is why threads are often preferred in case you need to often communicate in parallel (e.g. a GUI and its backend), they however increase the chance of bug since one misbehaving thread can disrupt the processing of all the other threads sharing the same address space in the application.Implement threadsNow that we know what threads are, how do we actually implement them ? There is a few models that exist for Kernel/User threads, namely: One to One model: One user-thread is matched against the same kernel-thread Better for synchronization and blocking issues Bad for performance (thread limits is quickly reached, and kernel is required for each operations) Many to One model: Multiple user-threads are matched against one kernel-thread Better for portability, and less dependency on kernel thread limits Bad If one user-thread blocks a kernel-thread Many to Many model: have bound/unbound threads Best implementation Requires coordination between user/kernel thread managers (namely pool) In order to understand them better, let’s study a few multithreading design patternsDesign patternsWe will first focus on a usecase where we receive a task, onto which we will apply a truly independent computation, and return the result. In the next part, we will try to understand how we can share this data, and perform thread-safe concurrency with lock and communication.I will be using Go language, because why not. It’s possible to have finer tuning by using C/C++ kthreads.h library, in order to create kernel threads on the fly for example. But they are extremely hard to use because most native data structures (e.g. Hashes or Dictionaries) will break. This post explains it very well.It is much easier to use user-threads (also called Green threads) as starter, as they are managed by your language, and chance are, they know what they are doing better than you (e.g. such as seeing partially-copied structure, transforming blocking calls into async). The best of both worlds is : use one OS thread per CPU core (since that’s your hardware limit anyway), and many green threads that can attach to any OS threads available. Languages like Go and Erlang provide this feature.Actors-based patternIn this model, each actor possesses a mailbox and can be addressed. They do not share a shared state, and when they need to communicate, they send a message and continue without blocking. Each actor will go through this mailbox and schedules each message received for execution. They also include a private state (occupied, etc.), and can make local decisions and create more actors.It is used by Akka for exampleWhy use them in threads if they are not going to have a shared state then ? Because It will be able to execute asynchronously and distributed by design. It alleviates the code from having to deal with explicit locking and thread management. There is a few implementations possible, namely thread-driven actors and event-driven actors.Boss/workers patternThis model is more popular, as It depicts a single thread (the boss) that acts as the scheduler and create workers on the fly. It is used notably by NodeJS and NGinx as It is easy to implement on an Event-based program. The boss accepts input for the entire program, and based on the input passes off tasks to one or more worker threads. He’s often the one that creates them, and wait for each of them to finish. There is multiple implementations, with or without a thread pool.Let’s create a simple program, which will receive a list of string, and return each bcrypt hash. Each hash calculation has to be done by an actor.The result is available hereIn the example, the iterative script takes ~ 50s to through 1000 words while the one with threads take ~ 100ms.As you can see, having threads can reduce dramatically your execution time if you need to have independent calculations.A few pointers on how to design them well: Identify truly independent computations Threads should be at the highest level possible (top down approach) Thread-safe data libraries is a must Stress-test your solution on multiple corner casesConcurrencyIn order to share data between threads, there is a few ways to communicate in a process, and It is often about picking one that best suits your needs.It’s possible to use IPC (Interprocess communication) between threads, but that could be considered overkill. It still remains one of the solution you could use to effectively communicate. A few examples to name them: gRPC Signals Pipes Message queues Semaphore Shared memoryBut a few mechanisms available only to threads exist, and It would be a shame to not use them.MutexThe first one is a sort of lock called mutex. Whenever the thread is accessing the data or resources that are shared among other threads, It locks a mutex so It can have exclusive access to the shared resources. The mutex usually contains a state (lock or free) and information about the thread locking it. What’s in-between the lock and unlock of mutex is called critical section, and that’s the place where you will safely access the resources. While It is a nice feature, you need to make sure your locking thread does not keep the mutex unavailable for too long, as It will make others threads unable to access the resource. This critical section needs to be as short as possible, as It is usually from where issues spawn.The mutex also possess a list of others threads that tried to access it, as It avoids starvation, where one thread is able to continuously access the mutex when another is not able to due to scheduling problems.This feature is often used in database, as It usually handles lots of concurrent access.Atomic variablesA shared mutable state very easily leads to problem when concurrency is involved. While using a lock such as mutex is nice, and allow to run through a critical section without being afraid of concurrency, It usually cause waiting issues on other threads.This is why atomic variables were created, in order to create non-blocking algorithms for concurrent environments. They ensure the data integrity by only allowing atomic instructions. These atomic operations focus on completing without any possibility for something to happen in-between, as they are indivisible and there is no way for a thread to slip through another one. There is no risk of data races, and allows better synchronization. They however are usually reserved for simple read or write of simple variables such as an Integer or String. If you need to concurrently access a file, a mutex would be fit better for this type of operation.Message passingBuilding a concurrent program is difficult, as It needs to take care of liveness (as less locks as possible) and fairness (each thread can process equally). A third type of communication is possible in Threads that tries to be as close as possible to these concepts is message passing. It can be implemented through several ways, but It is usually done in a synchronized queue. It re-uses the concept of producer-consumer pattern, but where each thread is able to be one or the other. The goal of the queue is to be as fair as possible, by including different scheduling with FIFO (First in First out) or FILO (First in Last Out). Each thread can implement a queue or they can share it through lock or atomic variables (pop and put).All these features try to ensure Thread safety and make sure your program runs smoothly.PitfallsA few common pitfalls stem from concurrent programming, and they come from the increased complexity of building a distributed architecture.The first one is usually the deadlock where two threads block each other resources: Thread A blocks resource 1 Thread B blocks resource 2 Thread A needs resource 2 to unlock resource 1 Thread B needs resource 1 to unlock resource 2The only way to avoid this is to reduce the critical section to the least possible amount of resources.The second one is race condition and It occurs when two or more threads can access shared data and they try to change it at the same time. Since you can not expect a specific order of execution due to the thread scheduling algorithm, you can not expect the change in data to be reliable as each thread will be “racing” to modify the data. The main issue is when a thread checks a value then act on it, even though another thread could have changed the value in-between.Here, you would typically put a lock or use atomic operations to ensure the data integrity.Third one is Starvation, where one thread occupies the majority of the execution scheduling, and It happens when there is no queue in place to ensure a thread does not wait too long, and increase in priority over time.Last one would be Livelock, where two or more threads keep on transferring states between on another instead of waiting infinitely. It usually happens when you try to “replay” messages when a part keep failing, and they stick to repeating the same pattern over and over again.ConclusionWhile concurrent programming is difficult to implement, and harder to maintain, It is still interesting to see how It can speed up your process and make use of the multi-core architecture we are now always using.The GO program was nice to implement, and the result is even nicer, and It really shows how good languages are getting at making parallel operations.I think I should tried implement more pitfalls to showcase how they can happen, but I don’t think they are too hard to understand concept-wise. They usually happen when an application become more and more complicated, and can be devastating in some case. Credits https://www.oreilly.com/library/view/the-art-of/9780596802424/ch04.html https://www.baeldung.com/java-deadlock-livelock https://applied-programming.github.io/Operating-Systems-Notes/3-Threads-and-Concurrency/" }, { "title": "Virtualization", "url": "/maxime-lair/posts/virtualization/", "categories": "RoadTo, OS Concepts", "tags": "linux, virtualization, iommu, container", "date": "2022-01-08 19:00:00 +0100", "snippet": "In this article, we will talk about the different types of virtualization available on the market, how they are implemented and try to provide an explanation how their rise to prominence in the past 15 years. At the end, we should understand what is a virtual machine, hypervisor, container or jails and have an idea of what’s coming next.Note I’m using a CentOS 9 Stream (released in Dec 2021) running on an ESXi 7.0.DefinitionVirtualization is the act of creating a virtual (as opposed to actual) version of something. It includes virtual computer hardware platforms, storage devices and computer network resources. While the term is broad, in our case It is mostly applied to a few different types, namely: Hardware virtualization Full virtualization Paravirtualization Desktop virtualization Operating-system-level virtualization, also known as containerizationWe will explore each of them, and understand their usecases and differences. There is many more types which can be applied to IT/CS, but It will be explored in other articles, to name a few (with some examples): Virtualization type Example Application Citrix XenApp Service Postman Memory AppFabric Caching Service Virtual memory Swap + RAM Storage disk partition Virtual file system CBFS/VFS Virtual disk .iso Data virtualization VDFS Network VLAN / vNIC / VPN Before we dive in the details, we can try to explain why It rose to such popularity in the recents years. By separating resources or requests for service from the physical delivery of that service, virtualization enables owners to distribute resources across the enterprise and use infrastructure more efficiently. This has been made evident with all Cloud computing platforms such as AWS, AZURE or GCP.The rise of the service design patterns made it all the more effective under Moore’s law, where computing power was rendered cheap, and Internet bandwidth exploded (through fiber or 5G networks), all to have clusters outsourced.Hardware virtualizationHardware virtualization specialize in efficiently employ underused physical hardware by allowing different computers to access a shared pool of resources. There is a few components to note: The hardware layer, often called host, contains the physical server components, It can be CPU, memory, network and disk drives. It requires an x86-based system with at least one CPU The hypervisor creates a virtualization layer that runs between the OS and the server hardware, and acts as a buffer between the host and the virtual machines. It isolates the virtual components from their physical counterparts Virtual machines are software emulations of a computing hardware environment, and provide the same functionalities of a physical computer. They are often called guest machine and consist of virtual hardware, guest OS and guest applicationsCPU virtualization emphasizes performance and runs directly on the processor whenever possible. The goal is to reduce the overhead when running instructions from the virtual layer compared to instructions on the hardware layer.IOMMU infrastructureHaving a memory controller with IOMMU will speed up virtualization instructions by reducing the amount of context switch, resulting in little to no difference compared to running hardware machine. This is often advertised as Intel VT-d or AMD-Vi. IOMMU is what made it all possible. It is an unit which allows guest virtual machines to directly use hardware devices through DMA and interrupt mapping. Be careful on this, as while this is a CPU unit, It requires motherboard chipset and system firmware (BIOS or UEFI) support to be usable.A note on SR-IOV - It is a chipset feature which allows scalability of devices on virtual platforms. In IOMMU, virtual devices are mapped directly to their physical devices for performance reasons, but It limits the number of virtual machine to your number of hardware devices. SR-IOV solve this by allowing splitting one PCI device to many virtual ones without performance drop via parallelized direct IO access.IOMMU goalIn a virtualization environment, the I/O operations of I/O devices of a (virtual) guest OS are translated by their hypervisor (software-based I/O address translation). It results naturally in a negative performance impact.In an emulation model, the hypervisor needs to manipulate interaction between the guest OS and the physical hardware. It implies that the hypervisor translates device address (from device-visible virtual address to device-visible physical address and back), this overhead requires more CPU computation power, and heavy I/O greatly impacts the system performance.The next figure illustrates it:Next, we get into the pass-through model, where the hypervisor is bypassed for the interaction between the guest OS and physical device. It has the advantage of avoiding the emulated device and attached driver. Here, the address translation is seamless between the guest OS and the physical device.The next figure illustrates it:It is made available thanks to a hardware-assisted component called IOMMU. And It looks more like this:There is two memory management units in a CPU: MMU (Memory management unit), to translate CPU-visible virtual address &amp;lt;-&amp;gt; physical address IOMMU (Input output memory management unit), to translate device-visible virtual address &amp;lt;-&amp;gt; physical addressIn order to provide this feature, IOMMU provides two functionalities, DMA remapping and interrupt remapping.IOMMU DMA remappingIn order to understand how DMA works, and why It is so effective, we need to do a recap of how memory works in our system.Physical memory is divided into discrete units called pages. Much of the system’s internal handling of memory is done on a per-page basis. Page size varies, but usually use 4kB pages.This means that If you look at a memory address, virtual or physical, It is divisible into a page number and an offset within the page. One example will explain it more easily how paging works:One note on TLB, since page tables are hold in memory, every data/instruction acccess requires 2 memory accesses (One for virtual address, one for physical address), and memory accesses are much slower than instruction execution in CPU. To accelerate the translation mechanism, a small fast-lookup hardware cache is added close to CPU, and this is called the Translation look-aside buffer or TLB, It contains the most common of the page-table entries. In the screenshot below, you will notice It is noted Huge, simply because 4KB is often not enough in today’s context, so we created bigger pages.Now that we know how memory is translated from virtual to physical realm, let’s dive into the DMA topic. DMA, or direct memory access, is the hardware mechanism that allows peripheral components to transfer their I/O data directly to and from main memory without the need to involve the system processor. A great deal of computational overhead is eliminated as the use of this mechanism can greatly increase throughput to and from a device.Without DMA, on any I/O operations, the CPU is typically fully occupied for the entire duration of the read and write operation, and is thus unavailable to perform other work. With DMA, the CPU first initiates the transfer then It does other operations while the transfer is in progress, and It finally receives an interrupt from the DMA controller when the operation is done. DMA does not only exist on CPU, but on many hardware systems such as disk drive controllers, graphics cards, network cards and sound cards.On PCI architecture, any PCI device can request control of the bus and request to read from and write to system memory. One issue is often the size of the address bus, as It can be unable to address memory above a certain line, and that’s where the IOMMU comes into play with its previously seen address translation mechanism.The idea of IOMMU DMA remapping is the same as the MMU for address translation.IOMMU interrupt remappingAn interrupt is a response by the CPU to an event that needs attention from the software. It is commonly used by hardware devices to indicate electronic or physical state changes that require time-sensitive attention.A MSI, or message signalled interrupts, are an alternative in-band method of signalling an interrupt. It allows devices to save up on an interrupt line (pin), as It uses in-band signalling to exchange special messages that indicates interrupts through the main data path. Fewer pins makes for a simpler, cheaper and more reliable connector. PCI Express only uses MSI for example as It presents a slight performance advantage.Device can trigger interrupt by performing a DMA to dedicated memory range (0xFEE00000 - 0xFEEFFFFF on x86). This means a virtual machine can program device to perform arbitrary interrupts. Without it, IOMMU cannot distinguish between genuine MSI from the device and a DMA pretending to be an interrupt.Full virtualizationNow that we understand how IOMMU came into play to enhance virtual machine performance, let’s check some relevant approach to virtualization technology.In Full virtualization, hardware is emulated to the extent that unmodified guest OS can run on the virtualization platform. Normally, this means that various hardware devices are emulated. Such virtualization platform attempts to run as many instructions on the native CPU as possible (which is a lot faster than CPU emulation). Many of these platforms require CPU extensions to assist virtualization such as an IOMMU.The hardware architecture is completely simulated, and the guest OS is unaware that It is in a virtualized environment, and therefore hardware is virtualized by the host OS so that the guest can issue commands to what It thinks is actual hardware. However, these are just simulated hardware devices created by the host, and the hypervisor translates all OS calls. It isolates VMs from the host OS and one another, enabling total portability of VMs between hosts regardless of underlying hardware.It is often called type-1 bare-metal virtualization. It offers the best isolation and security for virtual machines. A few products to name them: KVM, ESXi, Hyper-V or Xen.ParavirtualizationOS Assisted Virtualization is another approach to virtualization technology, where the guest OS is ported to the hypervisor, a layer sitting between the hardware and virtualized systems. Since It doesn’t require full device emulation or dynamic recompiling to catch privileged instructions, It is usually performing at a near-native speed.While the value proposition of paravirtualization is in lower virtualization overhead, its compatibility and portability is poor.It is often called type-2 virtualization. A few products use this technology, like QEMU, Xen, VirtualBox or VMWare workstation.Desktop virtualizationWhile not as popular as full-virtualization, desktop virtualization is a method of simulating a user workstation so It can be accessed from a remotely connected device. By abstracting the user desktop in this way, organizations can allow users to work from virtually anywhere with a network connecting to access enterprise resources without regard to the device or operating system employed by the remote user. It skyrocketed to popularity during the COVID pandemic and all the work-from-home habits.Since the user devices is basically a display, keyboard and mouse, a lost or stolen device presents a reduced risk to the organization. All user data and programs exist in the desktop virtualization server and not on client devices.There is three types of desktop virtualization: Virtual desktop infrastructure (VDI) - either on-premises or in the cloud, It manages the desktop virtualization server as they would any other application server Remote desktop services (RDS) - runs a limited number of virtualized applications which are streamed to the local device, offers a higher density of users per VM Desktop-as-a-service (DaaS) - shifts the burden of providing desktop virtualization to service providers, depends on IT expenses/needsContainerizationContainers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware. This makes containers more portable and efficient. They can be considered a lighter-weight, more agile way of handling virtualization since they don’t use a hypervisor.To re-use the previous figure, containers run like this:Containers are an abstraction at the application layer that packages code and dependencies together. They take up less space than VMs (typically tens of MBs in size) and can handle more applications. It is all from the benefits of reducing the operating system redundancy/overhead included in VM. Containerization packages together everything needed to run a single application (along with runtime libraries they need to run). The container includes all the code, its dependencies and even the operating system itself. This enables applications to run almost anywhere.Containers use a form of operating system virtualization, but they leverage features of the host operating system to isolate processes and control their access to physical devices. While the technology has been around for decades, the introduction of Docker in 2013 changed the common consensus.Containers are made available through a few Kernel features, mainly:| Kernel features | Description || — | — || Kernel namespaces | It wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource || Seccomp | Provides application sandboxing mechanism. It allows one to configure actions to take for matched syscalls || CGroups (control groups) | Used to restrict resource usage for a container and handle device access. It restrict cpu, memory, IO, pids, network and RDMA resources for the container |There is a few containers projects to note: LXC (System containers without the overhead of running a kernel) - also called Linux containers Docker containers (cross-platform, standalone executable packages) Snaps (Single machine deployment for fleet of IoT devices) Tanzu (VMWare container solution)It is interesting to note that, while Docker has been synonymous with containers from the beginning, It might change in the coming years. Kubernetes announced last year that they will shift from Docker Runtime to the Container Runtime Interface as defined by the Open Container Initiative, which supports a broader set of container runtimes with smooth interoperability. It will open the way for Docker competitors in the future (or not).Container managerFirst, check out this containerd architecture:It gives out a nice top-level overview of how a system (whether It’s Windows or Linux) interact with its containers (whether they are docker, from cloud providers, or from k8s).Why am I talking about containerd ? Docker (or containers) is a cluster of various utilities doing a wide variety of things hidden under the hood. Simply typing docker run webserver is great for users, but bad to understand its inner architecture. A great article about this is here and this part will simply reflect what I learnt from it.containerd is meant to be a simple daemon that will manage your containers and shims so they can run on any system. This manager will be the sticking glue between all your containers and the underlying system. It focuses on handling multiple containers so they can co-exist happily. It will handle all the boring part you don’t think of, like : Image push and pull support Interfaces creation, modification and deletion Management of network namespaces containers to join existing ones. Storing container logs and snapshots Support of container runtime and lifecyclePicture an apartment building. The hardware and system could be considered the ground, where utilities such as electricity (CPU), water (Storage) and heating (RAM) comes from. The building is the container manager, allowing each unit to co-exist by appointing each resource. Each apartment is the container runtime, that possess its own layout (configuration), and host a tenant. The container is this tenant who will use the available resource to conduct its lifecycle.Most interactions with the Linux and Windows container feature sets are handled a container runtime, often via runc and/or OS-specific libraries.Container runtimeAlso called OCI runtime, there is a specification for it to specify the configuration, execution environment and lifecycle of a container.For example, In each container, to name a few specification: In the filesystem, the following directory should be available: /proc /sys /dev/pts /dev/shm Following devices should be supplied: /dev/null /dev/zero /dev/full /dev/random /dev/tty Possess a status which may be : creating created running stopped Run through the following lifecycle: Create -&amp;gt; createRuntime -&amp;gt; createContainer -&amp;gt; startContainer -&amp;gt; delete -&amp;gt; poststopA container runtime can be considered the client part that will interact with its container manager. To start a containerized process, It happens in two steps: we create the container, then we run the process inside it. To create the container, we need to create namespaces (to isolate it from others), configure cgroups (to limit its ressource usage), etc. That’s what the container runtime will do. It knows how to create such boxes and how to interact with them since he created it.Some folks also call this brick : low-level container runtimes since they only handle container execution. High-level container runtimes would handle image format/management/sharing like containerd does, but It is just confusing, so I prefer to separate them into manager and runtime. It is difficult to name them on high/low scale, because docker runs containerd which runs runc, add a container orchestration tool on top of it, and you are stuck in a maze of naming conventions.You can even add programs that will stand in-between the container runtime and the container manager, they are called shim.An architecture top-level overview (correct me If I’m wrong, It’s difficult to place everything correctly.Linux containers A container is an isolated (namespaces) and restricted (cgroups, capabilities, seccomp) process.This phrase recaps what we learned of containers so far, but It is not necessarily true. In theory, they are an isolated and restricted environments to run on or many processes inside. This means projects like Kata implements container without using namespaces or cgroups but full-fledged VMs and be used by Kubernetes for example.Here, we will focus on the most popular kind of containers, which are Linux containers. Here we could use LXC to illustrate it, but let’s not forget that It is a set of user-land utilities, we can just try to use kernel features instead to reproduce it (not as well of course).Let’s create a cgroup and launch an application inside. Since we will be using kernel features, I will use root user for simplification.Let’s define what I want to run: &amp;lt;/dev/zero head -c 6G | tail - It will fill 6G of RAM on the systemI’m using cgroups v2 for this, be sure to check what your kernel version has with:The following script took me a few hours, as I had to realllyy read the kernel documentation on cgroups v2. Main issue was the No internal Process Constraint which make it so when you want to create a new cgroup and switch process inside it, you have to start from the root cgroup, because starting from any other cgroup requires you to switch all your running processes into the newly created one before doing anything (like adding memory control onto its subtree_control file).The script switch your shell into the root cgroup, run the command, switch the process into our new cgroup, limits RAM to 2G let it run for a few seconds (even though It should go up to 6G), you will notice how the memory will not go higher than 2G and will slowly start to fill in the swap space before we end it. It then re-switch back to the original cgroup. It’s not perfect of course, feel free to comment on it.The code :#!/bin/sh## ASSUMES CGROUPSv2# Store current cgroupshell_pid=$$stored_cgroup=&quot;$(cut -d&quot;:&quot; -f3 &quot;/proc/$shell_pid/cgroup&quot;)&quot;switched_cgroup=false# if we are not already inside / cgroup, switch usif ! grep &quot;\\b$shell_pid\\b&quot; &quot;/sys/fs/cgroup/cgroup.procs&quot; &amp;gt; /dev/null; then printf &quot;[I] Switching shell from %s cgroup to root cgroup\\n&quot; &quot;$stored_cgroup&quot; echo &quot;$shell_pid&quot; &amp;gt; &quot;/sys/fs/cgroup/cgroup.procs&quot; switched_cgroup=trueelse printf &quot;[I] Already in root cgroup\\n&quot;ficgroup_name=&quot;cg_test&quot;cgroup_dir=&quot;/sys/fs/cgroup/$cgroup_name&quot;byte_memory_input=6442450944 # 6 GBbyte_memory_limit=2147483648 # 2 GBif [ ! -d $cgroup_dir ]; then mkdir $cgroup_direlse printf &quot;[I] %s already exist\\n&quot; &quot;$cgroup_dir&quot;fi# Specify we want memoryif [ $(grep -i &quot;memory&quot; &quot;$cgroup_dir/../cgroup.controllers&quot; | wc -l) -eq 1 ]; then if [ ! $(grep -i &quot;memory&quot; &quot;$cgroup_dir/../cgroup.subtree_control&quot; | wc -l) -eq 1 ]; then printf &quot;[E] Missing memory in subtree parent control cgroup\\n&quot; exit 1 fi printf &quot;[I] Memory is available on parent cgroups, we can add limit on our cgroup\\n&quot;else printf &quot;[E] Memory unavailable on cgroups, check parent available resources in $cgroup_dir, exiting..\\n&quot; exit 1fi## BE CAREFUL TO NOT ADD ANYTHING INTO OUR NEWLY CREATED CGROUP SUBTREE_CONTROL## See https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#no-internal-process-constraint## It makes it impossible to add any process to our cgroup, since It stops being a leaf# Add our memory limit to the cgroup# memory.high is the memory usage throttle limit# memory.max is the memory usage hard limit, going beyond invokes OOM grimreaperif [ -f &quot;$cgroup_dir/memory.high&quot; ]; then printf &quot;[I] Adding memory limit of %s to our cgroup\\n&quot; &quot;$byte_memory_limit&quot; echo &quot;$byte_memory_limit&quot; &amp;gt;&amp;gt; &quot;$cgroup_dir/memory.max&quot;else printf &quot;[E] Missing %s\\n&quot; &quot;$cgroup_dir/memory.max&quot; printf &quot;[E] Unable to add memory limit, exiting..\\n&quot; exit 1fi# Now that our cgroup is created with a memory limit# We need to execute our program in its namespace and add its PID to our cgroups&amp;lt;/dev/zero head -c $byte_memory_input | tail &amp;amp;process_pid=$!printf &quot;[I] Created memory command at %s\\n&quot; &quot;$process_pid&quot;printf &quot;[I] Currently executing in cgroup %s\\n&quot; &quot;$(cut -d&quot;:&quot; -f3 &quot;/proc/$process_pid/cgroup&quot;)&quot;printf &quot;[I] Adding PID %s to %s/cgroup.procs\\n&quot; &quot;$process_pid&quot; &quot;$cgroup_dir&quot;printf &quot;[I] Switching process to our new cgroup %s\\n&quot; &quot;$cgroup_name&quot;echo &quot;$process_pid&quot; &amp;gt;&amp;gt; &quot;$cgroup_dir/cgroup.procs&quot;## Sleep for a bit, not necessary but better to read what&#39;s going onprintf &quot;[I] Going to sleep while you read this\\n&quot;sleep 15printf &quot;[I] Killing memory process as test is over\\n&quot;kill -9 &quot;$process_pid&quot;printf &quot;[I] Kill successfull, exiting soon\\n&quot;if [ &quot;$switched_cgroup&quot; = true ]; then printf &quot;[I] Switching back to original cgroup %s\\n&quot; echo &quot;$shell_pid&quot; &amp;gt; &quot;/sys/fs/cgroup$stored_cgroup/cgroup.procs&quot; printf &quot;[I] Now running shell in cgroup: %s\\n&quot; &quot;$(cut -d&quot;:&quot; -f3 &quot;/proc/$shell_pid/cgroup&quot;)&quot;else printf &quot;[I] We were already in root cgroup, just exiting without switching\\n&quot;fiexit 0The result:Feel free to add namespaces on top of it, use unshare command to add itThe next stepThe technology is still very new, as demonstrated with the google trends of Docker and K8:There is still many work to do in order to ensure containers monitoring, provisionning and orchestration. They are indeniably the way we will package applications in the future, as It is much more adaptable to a Cloud environment where hardware requires to be elastic to answer a growing organization needs.ConclusionThis concludes the introduction to virtualization as a concept. While I did not dive into the different linux technologies such as QEMU or Virt-lib, I was much more interested in understanding how IOMMU was working at the hardware level. I tried to avoid including too many details, and stayed with a top-level overview of each concepts. It’s a bit difficult to understand how they work architecturally, as many articles focus more on the code structure, or options. I’m happy with the page translation drawing I did, as It was not something clear to me for a long time. I hope It helped into understanding how virtualization is used, and how It will continue to evolve over the next decade. Credits https://www.citrix.com/fr-fr/solutions/vdi-and-daas/what-is-hardware-virtualization.html https://en.wikipedia.org/wiki/X86virtualization#I/O_MMU_virtualization(AMD-Vi_and_Intel_VT-d) https://lenovopress.com/lp1467.pdf https://www.cs.cornell.edu/courses/cs4410/2016su/slides/lecture11.pdf https://www.oreilly.com/library/view/linux-device-drivers/0596005903/ch15.html https://www.kernel.org/doc/html/latest/core-api/dma-api-howto.html https://www.infrapedia.com/app https://ubuntu.com/blog/what-is-virtualisation-the-basics https://www.docker.com https://containerd.io https://www.youtube.com/watch?v=sK5i-N34im8 // cgroups,namespaces and beyond: what are containers made from ? By J. Petazzoni https://jvns.ca/blog/2016/10/10/what-even-is-a-container/ https://iximiuz.com http://slides.com/chrisdown/avoiding-bash-pitfalls-and-code-smells" }, { "title": "Posix basics", "url": "/maxime-lair/posts/posix_basics/", "categories": "RoadTo, OS Concepts", "tags": "linux, bash, posix", "date": "2022-01-07 19:00:00 +0100", "snippet": "Note: I will be using CentOS 9 Stream which came out in Dec 2021. I will not be covering UNIX OS.We will dive in POSIX standardization, on the best practices for system interfaces, command interpreter and utilities. Our goal is to understand why POSIX exist, and how to apply their best practices.DefinitionPOSIX stands for Portable Operating System Interface and defines a set of standards to provide compatibility between different computing platforms.The latest version is available hereNot all operating systems are POSIX certified (such as Solaris or macOS), but they can be fully or partly POSIX compatible, as such, most OS try to be mostly POSIX-compliant.A few examples of POSIX-compliant: Android FreeBSD Linux VMWare ESXi CygwinWhen writing scripts/programs to rely on POSIX standards, you ensure to port them among a large family of Unix derivatives.Shell languageThe shell is a command language interpreter, which includes a syntax used by the sh utility.The difference between #!/bin/bash and #!/bin/sh is slim, as It spawns from the same binary:Using sh is simply having bash with --posix option after startup files are read. These startup files are the ones read and executed from the expanded ENV variable.It is not the case on every OS though, busybox can link sh to a different shell, but one common point they all have is having a /bin/sh. This is why in our case, It is better to use sh as a shell basis for our scripts.Variable expansionIt is possible to manipulate variable with prefix, suffix, default, fallback and message in portable shell syntax.Consider this shell script:It will print the variable value depending on its parametersWhile there is a few more, for example to search and replace, It is interesting to know It is POSIX compliant to do so.Be careful with unset as It is not compliant on arrays (only variables and functions) and declareEnvironment variablesEnvironment variables should always use UPPERCASE and underscore, like so:Only way to set an environment variable is through export, but they will only be available in the current session.Program exit statusStandard exit code is 0 for success, any number from 1 to 255 is to denote something else.To access the last exit code, you can use $?It is possible to suppress the exit code by nullifying STDERR and adding an OR, but this is not recommendedCommand line utility API conventionsStandard POSIX utilities includes an argument syntax to help them process.Unless otherwise noted, all utility descriptions use this notation, which is illustrated by this command:The utility is always named first, then comes the option-arguments, and includes two optionals modes : short options (single -) and long options (double --).In long options, parameters are added with --long_option=&amp;lt;PARAMETER&amp;gt;, while short options are one character-long and requires a between their parameters, e.g. `-s `.Optional arguments are then followed by mandatory arguments (called operands). You can include -- in-between to specify the ends of the optional-arguments, and helps in specific directory like systemd, where some filenames begin with -A few guidelines (or design pattern) on these utilities: Thou shalt not use more than nine characters or capital letters for their name Option-arguments should not be optionalWhile nothing is said about -h or -v, they are usually kept for help and version section.FilenamesFilenames cannot contain “/” nor ASCII NUL “\\0”. While this is flexible, a few more tiny limitations is necessary to be added upon the existing set of limitations.The character - is one to look out for, as It can lead to disaster.Let’s create a file named -rfAnd see what happens when someone try to remove all files in the directory with rm *, It should not destroy directories right ?Well.. not only did It destroy the directory, It also kept the -rf file. This could lead to bigger disaster.If you need to remove it, use --There is many more examples, but this is the reason why you need to ensure your file names should not start with - or control chars (such as \\n\\t).Directory structureMost linux distribution conform to FHS (Filesystem Hierarchy Standard), which defines a stricter set of rules to define the directory structure.POSIX defines a few guidelines on this structure: Applications should not be writing files in / or /dev /tmp should be made available for applications in need of temporary files creation. /dev/null is an infinite data sink, data written to or reads from this path should always return EOF /dev/tty Synonym for controlling terminal associated with the process group of that processRegular expressionPOSIX defines two regular expression syntax, called BRE (Basic) and ERE (Extended).BRE provides extensions to achieve consistency between utility programs such as grep or sed.In BRE, It defines the following syntax: Metacharacter Description . Matches any single character [ ] Matches a single character that is contained within the brackets [^ ] Matches a single character that is not contained within the brackets ^ Matches the starting position, if It is the first character of the regex $ Matches the ending position, if It is the last character of the regex * Matches the preceding element zero or more times \\{m\\} Matches the preceding element exactly m times \\( \\) Defines a capturing group, and treated as a single element While defining character classes that are used within brackets POSIX class similar to meaning [:upper:] [A-Z] uppercase letters [:lower:] [a-z] lowercase letters [:alpha:] [A-Za-z] upper- and lowercase letters [:digit:] [0-9] digits [:xdigit:] [0-9A-Fa-f] hexadecimal digits [:alnum:] [A-Za-z0-9] digits, upper- and lowercase letters [:punct:]   punctuation (all graphic characters except letters and digits) [:blank:] [ \\t] space and TAB characters only [:space:] [ \\t\\n\\r\\f\\v] blank (whitespace) characters And the more advanced extended regular expressions can sometimes be used with Unix utilities (grep -E, sed -E, or default in awk), the main difference is that some backlashes are removed, and non-greedy quantifiers (?)Shell syntaxLet’s use Shellcheck and test some commands to see how to write POSIX compliant code. It assumes you are somewhat familiar with shell scripting.The script:The execution:Shellsheck:In the end, some rules to remember are: Use test or single bracket for comparison. Use gt/lt for numbers, and avoid strings comparison with ==. Do not use arrays, and do not use declare let typeset or blank spaces when declaring variables Always quote strings, and use printf instead of echoConclusionWhile this ends POSIX basics, there is a lot to review in order to be POSIX compliant across all your shell scripts. The most difficult part is to know which tool is part of the core package, and which one requires an installation check.A few tools exist to check your syntax, like Checkbashims or Shellcheck. One tool, not really POSIX related, is useful for checking your bash commands is explainshellBeing POSIX compliant can be a pain (specially on grep, awk and sed implementations). It should serve as a common set of best practices for your scripts. Credits https://riptutorial.com/posix https://pubs.opengroup.org/onlinepubs/9699919799/toc.htm https://www.baeldung.com/linux/posix https://en.wikibooks.org/wiki/Regular_Expressions/POSIX-Extended_Regular_Expressions https://betterprogramming.pub/24-bashism-to-avoid-for-posix-compliant-shell-scripts-8e7c09e0f49a" }, { "title": "Startup management", "url": "/maxime-lair/posts/startup_management/", "categories": "RoadTo, OS Concepts", "tags": "linux, systemd, initd, unit", "date": "2022-01-06 19:00:00 +0100", "snippet": "In this article, we will take a look at how init management is used in Linux, first by defining it, understanding its behavior and configuration, and some useful commands.Note: Im running a CentOS9 Stream in this article, It came out in December 2021, I will not be covering UNIX or Windows, only LinuxWe will be focusing on the newer system systemd - we will not talk about its ancestor (sysV) or competitors (upstart..)DefinitionAll processes running on the system are child processes of the systemd init process. It is an init system used to boostrap you from the kernel to user space.systemd is rather new, as It was developped in 2010 and stable as of 2015. It is rapidly becoming the most used init process, but has some concerns on its inner security.It will be considered as a system and service manager. For those that haven’t noticed, It is a play on words refering to its quick adapting ability, but for real, the d in systemd refers to daemon, and If you need to refer to it, call it System daemon, not System D.It is hard to define systemd as a good or bad news for Linux. While It is praised by developers and users alike for its reliable parallelism during boot and centralized management of processes, daemons, services and mount points, It strayed away from the UNIX philosophy by its design (e.g. mission creep/bloat feeling).We will focus on the startup management part on systemd and the service process will be covered in another article.How does systemd starts ?Since RHEL 7 (Fedora 19+, Linux kernel 3.10.0-123), init scripts have been replaced with service units.While its predecessor sysV was using runlevels to define its startup process, systemd uses target units to start itself. Retro-compatibility wise, the system daemon has only limited support for runlevels, It has a number of target units that can be directly mapped to these runlevels, but not all can be directly mapped to runlevels. This is why It is recommended to avoid using the runlevel command, and also the reason we will not cover the different runlevels.Let’s focus on understanding how these units takes you from the boot phase to a useable system.Target units only purpose is to group together other systemd units through a chain of dependencies.To determine which target unit is used by default, we will use systemctl get-defaultThis represents the system end-goal to ensure our system is fully loaded, let’s check what It needs to be completed:Note: Unlike Requires, After does not explicitly activate the specified units. This means that to load the default.target, the system will only care about basic.target, and If we analyze it:Let’s check what system.slice has in store. A slice unit is a concept for hierarchically managing resources of a group of processes. It is performed by creating a node in the cgroup tree to limit resource applied to a specific slice.First, let’s understand what are cgroups, as they are an essential feature of the linux kernel (specially for containers).cgroupscgroup is the Linux Control Group that limits, accounts for, and isolates the resource usage (CPU, memory, disk, I/O, network, etc..) of a collection of processes. The first version was developped at Google in 2006 and released in the linux kernel mainline in 2008. The version 2 was merged in linux kernel 4.5 (Fedora uses it since ~ 2019), no RHEL released with it yet at this time, my Centos 9 is on linux kernel 5.14, so this is still very new.Main version difference is: unlike v1, cgroup v2 has only a single process hierarchy and discriminates between processes, not threads.One of the design goals of cgroups is to provide a unified interface, from controlling single processes (e.g. by using nice) to full operating system-level virtualization (e.g. LXC or docker), in order to provide: Resource limiting Prioritization Accounting ControlA control group (cgroup) is a collection of processes that are bound by the same criteria and associated with a set of parameters or limits. The kernel provides access to multiple controllers through the cgroup interface, these controllers can be Block IO, CPUSETS, HugeTLB, Memory, Network, Process number or RDMA.You can check these collections through systemd with two commands:systemd-cglssystemd-cgtopThere is also the tc command, a user-space utility program used to configure linux kernel packet scheduler.Various projects use cgroups as their basis, including Docker, Hadoop, Kubernetes and systemd.You can find your currently defined cgroups in /sys/fs/cgroup directoryNotice how we find back our cgroup defined by system.slice ; It itself will contain more cgroups defined in its hierarchyAnd its controllers (or subystem):System.sliceNow that we understand how cgroups is used to limit ressources usage per task groups, we can check how It works when our system starts up.We can check which units our system.slice is directly responsible for:system.slice is part of the four “.slice” units which form the basis of the hierarchy for assignment of resources for services, users and virtual machines/containers. Slice unit Description -.slice Root of the slice hierarchy, does not usually contain units directly but may be used to set defaults for the whole tree system.slice By default, all system services started by systemd are found in this slice user.slice By default, all user processes and services started on behalf of the user machine.slice By default, all virtual machines/containers registered with systemd-machined are found in this slice Bootup processNow that we understand how .slice units are used to control process resources through cgroups hierarchy tree, we go back to our default.target critical-chainAs a reminder, a usual bootup process goes as follow: Power-up System firmware (BIOS/UEFI/EFI, etc.) will do minimal hardware initialization Boot loader stored on a persistent storage device (systemd-boot for UEFI, GRUB2, etc.) to invoke OS kernel from disk (or network) The kernel will then mounts an in-memory file system, generated by dracut, usually an initramfs, and interprets it as a file system initrd hands over control to the host’s system manager (e.g. systemd) stored in the root file system systemd will probe all remaining hardware, mount all necessary file systems and spawn all configured servicesThe shutdown process does exactly the same as the bootup process, but in exact reverse.You can check your bootloader configuration in /boot/loader/entries/*-[kernel-version].confLet’s analyze each step to understand our bootup process. While this process is split up in various discrete steps, the boot-up process is highly parallelized so the order in which specific target units are reached is not deterministic, but still adheres to a limited amount of ordering structure.In order to reach multi-user.target, we go through a few different targets (and their dependencies), namely: local-fs-pre.target local-fs.target sysinit.target basic.target network-pre.target network-online.targetIf you wonder why this list is not the same as the one found in default.target configuration:This is because the critical-chain is a tree of required units, Wants is a weak dependency, if any of the wanted units does not start successfully, It has no impact on the default.target activation.Here is an incomplete view of some dependency for targets chain:Of course we are missing lots of service and others units, but If It helps to see how It enables parallelism.Change targetNow that we understand how our bootup process works in order to reach our default.target , how do we reverse that to make the system stop ?systemd gives you the ability to change to a different target unit in the current session (requires root).Just use systemctl isolate [target-name] to switch into the target unit, which will immediately stops all others non-required units (use at your own risk).This is useful in case we want to repair our system, in case It is unable to complete a regular booting process.For example, systemctl rescue allows you to switch to rescue.target, which provides a convenient single-user environment and mount all local file systems. It does not activate network interfaces or multiple user sessions. It ressembles systemctl isolate rescue.target but sends an informative message to all currently logged users in the system (wall).If you need to go further, you can use systemctl emergency, which provides the most minimal environment possible. It mounts the root file system only for reading, and does not attempt to mount any other local file systems.System power managementsystemd, as a system manager, will also handle the power management command whenever you want to stop, reboot, suspend it. Any commands you use is a symlink to systemctl utility.Each of these commands is a link to a target unit, which will try to stop the system as cleanly as possible.A few differences to note: halt terminates all processes and shuts down the cpu poweroff is similar to halt but also turns off the motherboard (lights, led, etc.) shutdown is similar to poweroff but runs shutdown scripts beforehand to stop things gracefully reboot is a shutdown and apply a hardware reset procedure so that the boot process takes over suspend saves the system state in RAM, and powers off most the device in the machine until It is turned back again hibernate saves the system state on the hard disk drive and powers off the machine until It is turned back againConclusionWe discovered how systemd takes you from the system bootup process to his default.target and how we can change this target to shutdown our system. There is much more to fully uncover this startup process, for example by applying cgroups to limit resources usage on a specific target.Feel free to check out the credits below, as there were helpful in uncovering the surrounding shroud around systemd startup process. Credits https://en.wikipedia.org/wiki/Systemd https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd https://www.computernetworkingnotes.com/linux-tutorials/ https://www.freedesktop.org/software/systemd/man/systemctl.html https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/cgroups.html https://systemd.io" }, { "title": "Service management", "url": "/maxime-lair/posts/service_management/", "categories": "RoadTo, OS Concepts", "tags": "linux, initd, service", "date": "2022-01-05 19:00:00 +0100", "snippet": "In this article, we will take a look at how service management is used in Linux, first by defining it, understanding its behavior and configuration, and some useful commands.Note: Im running a CentOS9 Stream in this article, It came out in December 2021, I will not be covering UNIX or Windows, only LinuxWe will be focusing on the newer system systemd - we will not talk about its ancestor (sysV) or competitors (upstart..)DefinitionAll processes running on the system are child processes of the systemd init process. It is an init system used to boostrap you from the kernel to user space.systemd is rather new, as It was developped in 2010 and stable as of 2015. It is rapidly becoming the most used init process, but has some concerns on its inner security.It will be considered as a system and service manager. For those that haven’t noticed, It is a play on words refering to its quick adapting ability, but for real, the d in systemd refers to daemon, and If you need to refer to it, call it System daemon, not System D.It is hard to define systemd as a good or bad news for Linux. While It is praised by developers and users alike for its reliable parallelism during boot and centralized management of processes, daemons, services and mount points, It strayed away from the UNIX philosophy by its design (e.g. mission creep/bloat feeling).We will focus on the service management part on systemd and the init process will be covered in another article.What does systemd manage ?First, It does NOT manage anything in /etc/init.d/. You should never add configuration there, as It will have no effect. There is backwards compatibility with SysV init scripts, but they will not be searched in this deprecated directory.You can explore /etc/systemd/ instead, It gives an idea of the inner architectureYou can analyze each file full configuration with systemd-analyze cat-config [PATH_TO_FILE]At its core, systemd manages units, and there is 11 available, highlighted in bold are the most important: Units Description Service Start and control daemons and processes they consist of, most used type Socket Encapsulate local IPC or network sockets in the system, useful for socket-based activation Target Useful to group other units through a chain of dependencies Device Expose kernel devices and may be used to implement device-based activation Mount Control mount points in the file system Automount For on-demand mounting of file systems (hot-plug..) Timer Useful for triggering activation of other units based on timers Swap Similar to mount units, encapsulate memory swap components of the operating system Path Used to activate other services when file system objects change Slice Group and manage processes and resources (cgroups) Scope Similar to service units, but for foreign processes management (e.g. init) Units are named after their configuration files, which can include positive or negative requirement dependencies as well as ordering.One example of such unit is the cron daemon, used for scheduling your batch.To put it into perspective, my CentOS uses 370 unit files, with 141 loaded unitsHow are units made ?Each units is configured through a plain-text files, with a .ini type syntax.systemd stores them in three location, depending on their usage Path Type of unit /usr/lib/systemd/system systemd default unit distributed by RPM packages /run/systemd/system Systemd unit files created at run time. This directory takes precedence over the default /etc/systemd/system Systemd unit files created by systemctl enable as well as unit files added for extending a service. This directory takes precedence over the run-time one An example of crond.service unit configuration, part of the default configuration:You can notice its scheduling dependencies on After= and WantedBy ; It is also able to restart itself on a specific timer.Note: It’s common case to have those service units configuration created in the three locations, and there is a link on higher priority paths towards the default one.Be careful when you change it, as anything written in the default unit path (in /usr/lib/) will be overwritten at each OS update.Well known unitsIf you are interested in systemd service architecture, try out systemctl list-dependenciesAmong all the running units (and not only services), there is a few interesting ones that you might already know: Unit Type Used for proc-sys-fs-binfmt_misc automount all block/character devices run-user-xxxx mount storing files used by running processes for that user sys-kernel-debug mount Kernel debug file system sys-kernel-tracing mount Kernel Trace file system init scope System and service manager auditd service Security auditing service crond service Command scheduler firewalld service Dynamic firewall daemon NetworkManager service Handles network configuration sshd service OpenSSH server daemon systemd-journald service Journal service systemd-logind service user login management user@xxxx service User manager for UID xxxx user slice User and session slice dbus socket D-Bus System Message Bus Socket network target Network component There is many more, and you can check a unit’s critical chain with systemd-analyze critical-chain [unit]If you need to have better boot time, you can check what take the longest.Possible state of unitsLet’s preface it by saying systemd does not communicate with services that have not been started by it. All is managed through a process PID, and used to query and manage the service. If you did not start your daemon through systemd, It will be impossible to query its state through systemd commands.There is two different types of state available on an unit: Loaded : If the unit file has been found, and It is enabled Active : If the unit is running or stoppedSince It is required to load an unit before running it, you need to consider both types to get a unit’s status.Unit loadingA unit has to be loaded if you need to run it. This state depends entirely on the unit configuration, and It can be one of the following: loaded not-found (e.g. not found in the three possibles paths) bad-setting error (e.g. set a masked service as enabled for example) maskedAn example of bad-setting, where we missed the initial / in the pathAfter correcting the issue:A note on mask : It is a stronger version of disable ; It links the unit file to /dev/null, making it impossible to start them. It prohibits all kinds of activation, including enablement or manual activation. It can be used to prevent accidentally using a service that conflicts with a running one.Once It is loaded, It will look in its configuration for its enablement state ([Install] section of the unit file). It hooks the unit into its various suggested places (e.g. the unit is automatically started on boot or when a particular kind of hardware is plugged in).Unit activationYour unit can be either started or stopped, but It can actually be more refined than that. A unit possesses two levels of activation state: High-level (often called active), and can take the following state: active inactive failed reloading activating deactivating Low-level called sub, whose values depend on unit-type specific detailed state of the unitYou can check what substates is available per unit type with systemctl --state=helpCreate our own service unitLet’s create our own service unit, which will simply repeat a ping/pong in a file. It will help us understand its state, and how to use a custom file as a service. We could imagine implementing a socket, device, or any other types of units.Two things to note before we start: System services are unable to read from the standard input stream, and when started, It connects its standard input to /dev/null to prevent any interaction. System services do not inherit any context (e.g. environment variables like HOME or PATH) from the invoking user and their session. It runs in a clean execution context. You can check this out by typing env for your local environment variables and systemctl show-environment for systemd environment variables in your shell.Now, let’s create our service unit, in /etc/systemd/systemWe will call it pingpong.service:We write a simple script, which repeatedly write ping/pong into a named pipeLet’s start it ! We check its status beforehandWe can see It created a pipe successfullyThe script is not perfect, as It will be blocked if someone else access this pipe, as read is blocking. The script location is also not perfect, as It depends on a non-sudo user, and represents a security risk, It would be better to review its permission and put it in /usr/local/binLet’s stop it, and check It deleted our named pipeAll good ! It’s also possible to create a unit template, if you want to create a skeleton of your units.Useful commandsTo reload and apply unit changes:systemctl daemon-reloadsystemctl restart [UNIT_NAME]To have an overview of overridden/modified unit files, use systemd-deltaTo view groupings of processes system-wise, use systemd-cgls Credits https://en.wikipedia.org/wiki/Systemd https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd https://www.computernetworkingnotes.com/linux-tutorials/ https://www.freedesktop.org/software/systemd/man/systemctl.html" }, { "title": "IO management", "url": "/maxime-lair/posts/io_management/", "categories": "RoadTo, OS Concepts", "tags": "linux, bus, driver, device", "date": "2022-01-04 19:00:00 +0100", "snippet": "In/Out management on Linux allows us to understand how data flows in Linux, from information coming from the network card, to the hard-drive, passing by CPU, RAM or others devices.All systems will always be limited by their weakest link, or low-hanging fruitWe will try to understand how to exploit this architecture to manage bottlenecks.Note: I’m using a Centos 9 stream which came out this month, only Linux will be covered here.DefinitionTo make a computer work properly, buses must be provided that let information flow between CPU(s), RAM and others devices that are connected to the system.Devices can be purely an input type (e.g. keyboard or mouse), an output type (e.g. printer, display screen) or both an input and output device (e.g. disk)Regardless of their purpose, they must communicate through the system bus (e.g. typically the PCI), but they are not always directly connected to it. They can flow through interfaces first.Interfaces were created to easen the CPU’s load to avoid having to understand and respond to each and every device. Also, sending signals on this system bus requires a very low electrical power, which makes the connecting cable very short (a few centimeters).Interfaces are also called I/O controllers, and they typically holds three types of internal registers: data, command and status.Besides the system bus, several other types exists, like ISA or USB, that communicate through bridges.An image speak louder than words:Here, the bus connecting to the CPU is often called I/O bus.ConceptsAccessing I/O devicesIt is possible to have direct control of any I/O devices through their controllers, but this would lead to a multitude of problems (unintentionnal or malicious).To avoid these problems, Linux provide routines to conveniently access those devices, made of system calls. Linux provides an API which abstracts peforming IO across all busses and devices, allowing device drivers to be written independently of bus type.Each device connected to the I/O bus has its own set of addresses which are called I/O ports. The CPU selects the required I/O port and transfers the data between a CPU register and the port.Memory mapped IOI/O ports may be mapped into addresses of the physical address space. The CPU is then able to issue an assembly instruction (mov, and, etc.) that operate directly on memory.It is the most widely supported form of I/O because It is faster and can be combined with DMA.DMA was created for modern bus architecture, allowing an I/O device to transfer data directly from RAM. Only the CPU can activate this for each device and setup time is relatively high, but allows a device to be independent from the CPU after being authorized. It is mostly used by devices who need to transfer a large number of data at once.There is synchronous and insynchronous DMA, the first is triggered by processes and the second by hardware devices.To go further, there is a memory management unit to connect I/O bus to the main memory, called IOMMUIt is notably used when running an operating system inside a virtual machine. It allows guest virtual machines to directly use peripheral devices such as Ethernet through DMA. AMD calls it “AMD-Vi” and Intel “VT-d”.Devices driversA device driver is the set of kernel routines that makes a hardware device respond to theprogramming interface defined by the canonical set of VFS functions (open, read,lseek, ioctl, and so forth) that control a device.Linux I/ONow that we have seen how a device is considered from hardware to being present in the file system, let us see how a user can input and data with it !A device driver acts as the bridge between the device (and its controller) and kernel modules. The main concept is a command should be device independent, we do not need to specify the I/O device to interact with it.The way we can interact with those devices depend on its type, and there is a few to remember: Block devices, focusing on large data transfer (e.g. a hard drive) Character devices, focusing on small and quick data transfer (e.g. a network card) Clock devices, focusing on quick data accessOne principle is to balance CPU, memory, bus and I/O operations, so a bottleneck in one does not idle all the others. One key aspect in controlling I/O speed is to control the number of context switch, but It comes with increased development cost and abstractions. Going from an application code running in userland to the device code (so It went through kernel, device driver, device controller) can take a long time (See ASIC or FPGA)Useful commandsIn order to find bottlenecks in our I/O limitations, we can try to use a few utilities packaged in Linux.One of the best utilities is sar which collect most I/O activitiesWith more informations:You will notice a lot of attributes, one metric is great at identifying bottleneck: iowaitIt represents the percentage of time the CPU had to wait before reading or writing data.We can also use iotop to check those informations. However this type of information is not available by default, and It seems there is a kernel issue in the latest distribution, as enabling it does nothing. Using iotop -aoP would have shown us how much a process has written and read since iotop started, too bad.Let’s try a command that will flood the system, and see how It reactscat /dev/zero |head -c 5G | tailLet’s check how many context switch we have when we use itHere we can notice how It went up for 4 seconds (when the command was executed), as It needed time to move data across devices, but then went down once this data was transferred into memory.It can report statistics on network, CPUs, block devices and much more.Multiple other utilities exists, like vmstat or dstatOr if you are feeling more adventurous, you can try to parse through /sys/block/sda/stat or /proc/diskstatsTo go furtherThere is a LOT to explain on the bus/device controller/driver. I didn’t take the time to get through interrupt, polling and much more. I feel like I did not fully understand how bus works, as It’s hard to make the link between hardware and software, there is no visual to work from. I think I should have added notions of south and north bridge, but I don’t even know if they are still used or integrated into the CPU directly.Check out the credits below, they are an excellent source of information to dive deeper into this subject. Credits https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit https://doc.lagout.org/operating%20system%20/linux/Understanding%20Linux%20Kernel.pdf https://applied-programming.github.io/Operating-Systems-Notes/8-IO-Management/#io-management https://www.oreilly.com/library/view/understanding-the-linux/0596002130/ch13.html https://www.kernel.org/doc/html/latest/driver-api/device-io.html https://www.redhat.com/sysadmin/io-reporting-linux https://unix.stackexchange.com/questions/55212/how-can-i-monitor-disk-io https://haydenjames.io/linux-server-performance-disk-io-slowing-application/ https://www.linuxtopia.org/online_books/introduction_to_linux/linux_I_O_resources.html https://www.slashroot.in/linux-system-io-monitoring" }, { "title": "Memory and storage", "url": "/maxime-lair/posts/memory_and_storage/", "categories": "RoadTo, OS Concepts", "tags": "linux, memory, storage", "date": "2022-01-03 19:00:00 +0100", "snippet": "In this article, we will take a look at how memory and storage interact in Linux, the different technology available, and their perks and defaultsNote: Im running a CentOS9 Stream in this article, It came out this very month, I will not be covering UNIX or Windows, only LinuxPhysical memoryDefinitionWe will try to focus on the different type of memory in Linux.While there is the physical hardware that makes up RAM, and the virtual memory you see on Linux.RAM was made to be performant, quick access on read and write, while disk can focus on long-term memory.Types of RAMMost RAM today are of DDR-RAM type, which stands for Double Data Rate and exist since 2002. Compared to a single data rate, Its performance is doubled (up to 3 GB/s) as It was able to read data on both the rising and falling edges of the clock signal.DDR2 was then invented in 2004, delivering up to 6.5 GB/s bandwidth. It introduced the dual-channel memory concept, where CPUs could communicate using a bus with the memory module through independent channels, essentially amping up the bandwidth as data could be sent on more than one channel.Today, servers are usually designed with six to eight channels.The most important part is : you can not use more than one type of memory per motherboard, they can only handle on type of memory (DDR2 or DDR3 or else..).In reality, It goes even further than that as you only use memories of the same type, manufacturer and frequency.Each DDR iteration made memory faster and more energy efficient. DDR3, which came out in 2007, made bandwidth go up to 15 GB/s.Today, most servers are using DDR4, pushing up to 25GB/s, and we are slowly shifting towards DDR5, released in 2020, with 50 GB/s bandwidth.If you try to get DDR5 memory now, It will probably cost you a lot of more than for DDR4, as It is still difficult to ramp up into mass-production.Note: There is also VRAM but It is only used in graphic cards, and It is a lot more expensive.You might also notice some memory marked as ECC which stands for Error correction code. ECC memory maintains a memory system immune to single-bit errors.It decreases your memory overall performance by 2-3% and cost more, but makes your memory a lot more reliable. Mostly used in database or file servers, It reduces the number of crashes due to memory data corruption.On Linux, you can check the physical chip information with dmidecode - type 17 is memory deviceIf you are not happy with DDR memory, you can upgrade to HBM memory. They will be able to handle ~ 300 GB/s but they do not come cheap.Virtual memoryDefinitionAs system becomes more greedy in memory usage and RAM becoming more scarce, Linux had to think about ways to make the system reproduce RAM virtually.Virtual memory is using a disk as an extension of RAM, so the effective size of usable memory grows correspondingly.Of course, reading and writing on the hard disk is thousand times slower than using physical memory, but with the rise of SSD, this technology is becoming more and more efficient.The part of the hard disk used as virtual memory is called swap space. It can be a normal file or a separate partition (recommended).While using a file allows for more flexibility, as you dont need to reformat it, the current standard is to use LVM data partition together with a swap partition.Like so:Linux memory management is responsible for : Implementation of virtual memory Demand paging Memory allocation both for kernel internal structures and user space programs Mapping files into processes address space and much more !Note Swapping means writing the whole process out to swap space, while paging is writing only fixed size parts (a few kb) and is more efficient.BufferingShared memory is when multiple process use the same memory block.Buffers/cache represents the disk buffer cache.As reading from a disk is very slow compared to physical memory, It is common to save commonly accessed data onto the memory until no longer needed.It speeds up all but the first read (when you first access it on the disk), and It is called disk buffering while the memory used for this purpose is called buffer cacheIf this buffer cache is full, the data unused for the longest time is discarded first to free up some memory.This is important as this disk buffering works for writes as well, and is the cause of lost data when the system crash as It has no time to save it onto the disk.You can force this write with sync or bdflush (automatic on Linux every 30 seconds)This cache size is automatically set by Linux and is usually made up of all free RAM. It decreases in size when memory is needed for greedier process.PagingPhysical memory is not necessarily contiguous, It can be accessible as a set of distinct adress ranges. All this makes dealing directly with physical memory quite the hassle and to avoid this complexity a concept of virtual memory was developed.Paging is the bridge between this virtual and physical address. Each time a software requires to write, the virtual memory only share the needed information in the physical memory (demand paging), while providing a mechanism for protecting and controlling sharing of data between processes.Each physical memory page can be mapped as one or more virtual pages, and these mappings are described by page tables to allow translation from a virtual to physical memory address.You can check this table in /proc/meminfoTo avoid using CPUs for each address translation, modern architecture use a TLB or Translation Lookaside Buffer, to keep this information cached.We could dive deeper in the subject, but we just need to know this created the concept of Huge page which is mapped by either TLB or THPCommandsTo know your memory, you can check with freeIt will print you informations about your memory usage, notice how Swap is included, although It’s written on disk. RAM is usually called physical memory.This information is polled from /proc/meminfo ; but some others informations are available in /sys/devices/system/memory/We can test this memory usage with several command:cat /dev/zero |head -c 5G | tailtail /dev/zerocat &amp;lt;( &amp;lt;/dev/zero head -c 500m) &amp;lt;(sleep 5) | tailLet’s try to fill out memory and swap space ! First we fill our physical memory (8G) and swap (5G) with 10G of data.then we check our memory is used with top:or with freeAnd after we stop it, we are back to normal:You can also look into pv command to have an overtime change on RAM (whether in rate/sec or minutes)StorageNow that we have seen how physical and virtual memory interacts, we will dive into long-term memory, the one that stays after each reboot.DefinitionLinux storage is based on block devices, providing buffered access to the hardware. While there is multitude of storage solutions now, whether they are cloud based, an external device or internal, they all share the same objective: storing data.It can used to save files, run tasks and applications, and today came to be in gigabytes or terabytes capacity.We will not cover USB flash drive, optical storage such as Blu-Ray discs, as there are decreasingly useful with the advent of mobile phones and cloud solutions.Different types of storageHard-driveAn HDD (Hard disk drive) is a hard drive that uses mechanical spinning disks using a magnetic tip to read and write data.HDDs are considered as the most reliable data storage hardware in most servers, the main appeal to get them are their cheap cost compared to their capacity, although they are slower than others solutions.They are often called SATA drive, as this is the name of the computer bus interface that connects them to the motherboard.It’s not wrong to say that, but SSDs can also be connected to the motherboard through the SATA interface.To bring a few words on SATA, there is today others solutions like Thunderbolt, SCSI, NVMe. SATA has 3 versions (I II III), going from 150 MB/s, 300 MB/s to 600MB/s, however most hard-drive are unable to beyond 150 MB/s speed.This is the reason why SATA drive are an idiom for hard-drive disk, even though SATA III could handle some SSD speed.Server hard drive works 24/7/365, and usually comes in with a 3-5 years warranty. This makes them way more pricer, but manufacturer often categorize them into three: ECO - consumer-grade hard-drive BC - pro-grade hard-drive EP - server-grade hard-driveThese hard-drives have differents requirements, for reliability, recovery and speed. Most benchmark will try to compare their sequential and random write and read speeds, but price and capacity should be equally considered. There is never too many HDDs.SSDWhile HDDs are mechanical disks, SSD are flash storage and are much faster (boot time is 10s). They are more expensive, and data recovery can be complicated. It is however easier to transport (no moving parts), and consume less power. Most manufacturers are now focusing primarly on SSDs instead of HDDs.SSDs are usually limited to their bus speed, in case of SATA III at around 450 MB/s sequential speed making them 3 times faster than HDDs. For comparaison, USB3 has transmission speed up to 600 MB/s while USB2 are limited to 50 MB/s.This is the reason why most SSDs are now connected through PCIe (3.0 or 4.0) or NVMe, as they are able to scale way better.PCIe depends on their version (currently from 1 to 6) and their number of channels. x16 being the usual amount. PCIe version PCIe x16 speed (GB/s) 1.x 4 2.x 8 3.x 16 4.x 32 5.x 63 6.x 126 With SSD, going from SATA to PCIe-6-x16 (which comes out next year), would make it go 210 times faster. Of course, you would be then limited by your RAM or CPU speed.NVMe is a communication interface standing between your CPU and your storage interface using PCIe sockets. It was solely designed for SSD. While there is a few SSDs format, such as 2.5” or mSATA, the only one compatible with NVMe version are M.2 and U.2. Main differences between these last two formats is U.2 allows for hot-swap, you can add them during your system runtime.If you are still not satisfied with this, check out Optane.Cool commandsNow that we understand the different types of storages, Linux allows us to check their health through a few commandsYou could use df to report file system disk space usage. But It doesn’t show you your physical hard drives.If you are more interested in which files takes the most space, du is your tool:More focused on drives ? Use fdisk ir lsblk:Oh you meant actual hardware informations on those disks ? Use lshw -class diskIf you want to dive deeper, you might want to check out the available informations in /dev/disk where you can sort by:or in /sys/blockTo go furtherIf you are running another OS, check out rosetta to help you find the right commands.There was a lot to cover on this one, but I learned a lot on the virtual memory and bus interface, so all worth it ! Credits https://thewiredshopper.com/ddr3-vs-ddr4-vs-ddr5/ https://en.wikipedia.org/wiki/ECC_memory https://0xax.gitbooks.io/linux-insides/content/Theory/linux-theory-1.html https://tldp.org/LDP/sag/html/buffer-cache.html https://www.kernel.org/doc/html/latest/admin-guide/mm/concepts.html https://www.redhat.com/sysadmin/dissecting-free-command" }, { "title": "File systems", "url": "/maxime-lair/posts/file_systems/", "categories": "RoadTo, OS Concepts", "tags": "linux, filesystem, pipe, mount", "date": "2022-01-02 19:00:00 +0100", "snippet": "In this article, we will dive into the linux file systems, from the different format, its structure, the file types and some facts about partitions and mounts.My lab is running on a Centos 9 Stream, which came out this very month. Most of this will apply to Linux distribution, I will not cover Unix or Windows command.DefinitionFrom TLDP: A simple description of the UNIX system, also applicable to Linux, is this: “On a UNIX system, everything is a file; if something is not a file, it is a process.”This statement is true because there are special files that are more than just files (named pipes and sockets, for instance), but to keep things simple, saying that everything is a file is an acceptable generalization. A Linux system, just like UNIX, makes no difference between a file and a directory, since a directory is just a file containing names of other files. Programs, services, texts, images, and so forth, are all files. Input and output devices, and generally all devices, are considered to be files, according to the system.In order to manage all those files in an orderly fashion, man likes to think of them in an ordered tree-like structure on the hard disk, as we know from MS-DOS (Disk Operating System) for instance. The large branches contain more branches, and the branches at the end contain the tree’s leaves or normal files. For now we will use this image of the tree, but we will find out later why this is not a fully accurate image.This way of thinking in tree is redundant in OOP (Metholds are held by Class that implements Interfaces), blockchain or version control (Git branches).Different file systemBefore storing any data on your file system, your storage device must first be formatted to a type understood by the operating system. There is too many formats to name them, but we can start with the most popular ones: F2FS Ext4 XFS BtrfsThey all have their perks and defaults (stability, performance, OS support, etc.), you will use one depending on your usecase, there is bad and good answers as long as they are reasoned.We can check our own file format with fdiskAnd check on a specific partition which format to use is available:Flash media (F2FS)Without diving too deep, all you need to know is there is two types of flash memory: NOR and NAND logic. They share the same cell design, but are connected differently, which makes NOR able to random access and NAND restrained to page access. This makes NOR 60% physically larger than NAND flash. As a result, NOR is better for reading but worse for storing data, while NAND is quick at writing but less endurant.Now that you know that, you will understand why a specific file system format is better for some storage device than others.Let’s say you have a NAND flash memory-based storage devices like a SSD or SD Card. It is not surprising as they are becoming the most used storage device for servers and mobile devices due to their speed, noise and reliability. From what we just learned, they are quick to write into, but last less time. Can we use a file system that address their issues while supporting its strenghts ?That’s what Samsung did with F2FS, but It is hard to predict if they will become the most common filesystem for flash devices.Ext4This filesystem format stands for Extended File system and It is the default one in recent Linux. It is backwards compatible with former iterations (ext3, etc.) and useable in Windows 10 since 2016 and macOS through extFS.It released in 2008 and has stood as one of the most used file system format for more than a decade now. Google is one of their main users since 2010. This is why It is the default format for android phones today. My (humble) guess is ext4 will slowly (~ 10 years) decline to make room for more modern file systems format.It supports up to 1 Exabyte volumes (1000 To) and file size up to 16 Terabytes (TB).XFSThis format will probably overthrow ext4, as It was built for performance and is mainly used on storage that are constantly reading and writing data.To compare it with ext4, XFS supports up to 16 Exabytes volumes and 8 Exabytes file size (i.e. 16 times larger and 500 times larger than ext4). It also is optimized for quicker crash recovery thanks to its metadata journaling.It is supported by most operating systems, and can be used in google cloud for container-optimized OS.BtrFSAs the last file system format we check, with previous ones focused on flash, wide-support and performance, we will study one mainly used for long-term storage (for NAS, etc).It includes snapshots and self-healing mechanisms. Facebook is one of their users, much like Parrot project and Tripadvisor. It is the least popular ones among the four, as It is one of the most recent (~ stable as of 2013). As soon as people starts to use it and understand the snapshot perks, It might gain more traction in the future.Linux file system structureNow that we have seen some filesystem format, we can focus on the linux file system structure.Once you decide on F2FS, ext4, XFS or BtrFS format, linux will build its directory tree. It is defined by the FHS (Filesystem Hierarchy standard) but It is a “trailing standard” for most distributions.For example, all Linux distribution share the same root directory called / and essentials binaries needed in single-user mode are located in /bin. But they don’t all possess a /run directory.In CentOS 9:Note All symlink (bin, sbin, lib, lib64) points to /usrLet’s check each subdirectories, and understand what type of files we could find thereUseful toolsTo help us navigate through this filesystem, we can use some nice external utilities that nicely print file systems like: nnn (C based) xplr (made in Rust) midnight_commander (C based) ranger (made in Python)These tools are optional, but they can help having a better top-level view of the structure.Root subdirectories Subdirectory Description /bin Contains binaries (Over 260M and 1300 files), this directory has evolved overtime. In the past, It contained the minimum number of binaries needed to operate the system (like busybox). Now, It’s merged with non-essential binaries (multi-user mode), and /bin is just a symlink for backwards compatibility. You should never install a binary here, only the ones provided by the OS. Prefer using /usr/local/bin /boot Contains boot loader files responsible for booting the server (efi, grub, loader and the kernel vmlinuz), this directory is critical for loading linux on reboot /dev Stands for device, lists references to all CPU peripheral hardware, which are files with special properties, tty are for terminals (USB, serial port), sd for mass-storage driver, mem for main memory, hd for IDE driver, and pt for pseudo-terminals. There is three types: character, block and pseudo devices. /etc Host-specific system-wide configuration files, It only contains static files. One notable subdirectory is /etc/opt used for add-on configuration packages /home Users home directories, containing all their saved files and settings /lib Libraries used by binaries and compilers (like libncurses or motd). You can check what libraries a binary uses with ldd command /media Mount points for removable media such as CD-ROMs, USB drives /mnt Temporarily mounted filesystems, you can create mount points anywhere in the system, but It is standard convention (and sheer praticality) to not litter the file system with randomly placed mount points /opt Used for third-party application without dependency outside of their package, It is supposed to be self-contained /proc Virtual filesystem (only available at runtime) providing process and kernel information as files, usually a procfs which create a hierarchical file-like structure used to dynamically access process data held in the kernel without using tracing methods or direct access to kernel memory /root Home directory of your system boss (root user) /run Recent subdirectory, It holds run-time variable data. It contains information about the running system since last boot, like currently logged-in users and running daemons. They are removed or truncated at the next boot process /sbin Contains essential system binaries, used for maintenance or administrative tasks. Locally installed binaries should be sent to /usr/local/sbin. We may find fdisk, lilo, init, ip, and interestingly enough, we may find some these binaries in /etc in older distributions /srv Holds site-specific data which is served by the system (ftp, rsync, www, cvs) /sys It is a virtual file system that can be accessed to set or obtain information about the kernel’s view of the system. It is different from /dev as the latter contains the actual devices files without going through the kernel, while /sys is an interface to the kernel, much like /proc. It contains informations on drivers, busses, network and much more. Also /proc is older and less structured. /tmp Contains files that are required temporarily, like lock files. It is one of the few subdirectory of / that does not require to be superuser /usr Usually contains by far the largest share of data on a system (Here 3G). It contains user binaries (in /usr/bin), libraries (/usr/lib), etc. It is deemed as shareable, read-only data and must not be written to. Any information that is host specific or varies with time is stored elsewhere. /var Contains variable data like system logging files, mail and printer spool directories. Some portions are not shareable between systems, like /var/log or /var/lock, while other may be shared, like /var/mail or /var/spool/news. Some directories are more critical than others, and can be put onto separate partitions or systems, for easier backup, due to network topology or security concerns.‘Mountable’ (Non-critical) directories are: /home /mnt /tmp /var Essential directories for booting are : /usr /boot /dev /etc /lib and /procFile typesNow that we know what each directory purpose is, we can take a look further into the file system. After directory comes files, so let’s check each of them !Before diving deeper, we need to understand the concept of inodeInodeLinux filesystems are hard to understand, especially for a computer working with 0 and 1. How does our system understand its structure ? That’s when an inode is essential, It’s an index node that describes the file attributes such as the physical location on the hard drive, permission privileges, in short all the metadata that we know of a file. It is stored separately from the files themselves. Each filesystem generate its inode stable to identify each file It holds.For every file on your file system, there is an inode containing 16 KB of this file metadata. We can check this number with dfHow do we check a file’s inode ?Here we can see each file types possess an inode, even links or directory.Of course there is more advanced operations available on an Inode, but know for example that an inode structure depends on its file type, like a directory holding a list of inodes of the files It contains.Regular fileA regular file can be created with touch and have no special attributesDirectoryWe can create a directory with mkdir which can be later recognized with the attribute dAnd we can check its inode and file attribute with statLinkLinks are created with ln, and there is two types: Hard link (default) - the new file will share the inode number with the original fileIt is only possible to hardlink a regular file, not a directory. Symbolic ( -s parameter) - It creates a new file (with its own inode) pointing to the original file addressSymbolic links can be applied to all types of files.And if we try to access its contents, It makes it appear as if we are accessing the real file:Notice how the inode is identical on the regular.file and hardlink (101631049) but different on the symlink (101631050)If the original file (regular.file) was to move somewhere else, the symlink would break (as the file address changed), but the hardlink would still point to the file as It retains his inode.Whether we move the file, or change its content, the hardlink will still access the right file contentHardlinks will only break if the file is moved to another volume.While symbolic links will only break if the original file is moved or deleted, but It can be used to reference across volumes.Hardlinks are less and less present in Linux, as they mostly shift towards symbolic links, but can still be encountered in smaller distribution like BusyBox, as It saves space on the drive.Special fileAlso named device file, It is an interface for a device driver that appear in a file system as It It were an ordinary file. They are three types: Block devices - works with fixed-size blocks (see buffers) and large output of data, such as with a hard-driveYou can find them with lsblk Character devices - can only send one character at a time, pratical in an internet connectionI could not find any command for finding characters devices (unless you want to search the entire file structure), but you can find them in /sys/dev/char/ Pseudo-devices - It is a device driver without an actual deviceThey serve pratical purpose, like a virtual sinkhole, or producing random dataSocketSockets are used to communicate between programs. There is two types: Stream sockets (using TCP as their underlying transport protocol) Datagram sockets (using UDP) Unix Domain Sockets (Using IPC - SOCK_STREAM)You can find them by checking your opened sockets with ss (netstat replacement) in /proc/{PID}/fdOr look for open files with lsofAlternatively, you can use find on specific file type (here named pipe and sockets)Named pipeA pipe has a read end and a write end. Data written to the write end of a pipe can be read from the read end of the pipe.As opposed to unnamed pipe, like the one to string together commands likeA named pipe, also known as First In First Out, is similar to a pipe but with a name on the filesystem.Processes can access this special file for reading and writing, and provides bidirectional communication.It is created with mkfifo and mknod (p for FIFO)And now we can simply use them to transfer data by specifying the pipe name. Here we use two shells, one for sending the data, the other to receive and compress it.PartitionsA hard disk can be divided into several partitions that will function as If It were a separate hard disk. The idea is that if you have one hard disk, and you want two operating systems on it, you can divide the disk into two. This information is stored in its first sector, also called Master boot record (MBR), of the disk. This is the sector the BIOS reads in and starts when the machine is first booted. This MBR contains the partition table to check which partition is active (i.e. bootable) and reads the partition’s boot sector (in case of Linux, its /boot)We can list our partitions with lsblkWe can see our disk sda is split into two partitions: sda1 (containing the first sector /boot) sda2 (containing our 49G data and swap) cs-root (containing our / filesystem) cs-swap (containing our swap volume) Disks are automatically named sd{a..b..c…z}LVM just means logical volume manager, used to create logical storage volumes with greater flexibility than partitions, such as resizeable storage pools to extend or reduce the volume size without reformatting or repartitioning the underlying disk devices.Also, a partition can be a primary partition of extended partitions. Consider just main partition and sub-partition if this vocabulary is too complex, they were just made like this because original partitioning scheme for hard disks were limited to four (primary) partitions.Data partitionConsidered a normal Linux system data, including the root partition containing all the data to start up and run the systemWe will often find a few partitions on the system: / ~ 3-5 GB+ and only ext4 on RHEL distrib /boot ~ 250 MB+ and only ext4 on RHEL distrib /home ~ 100 MB+Swap partitionExpansion of the computer’s physical memory, they are used to support virtual memory, and data is written to a swap partition when there is not enough RAM to store the data your system is processing.Over the past decade, the recommendend amount of swap space increased linearly with the amount of RAM in the system.The recommended swap space is as follow: Amount of RAM in the system Recommended swap space &amp;lt; 2GB 2 times the amount of RAM 2-8GB Equal the amount of RAM 8+GB At least 4 GB MountsAll partitions are attached to the system via a mount point. The mount point defines the place of a particular data set in the file system. They can be connected anywhere on the file system, but It is bad practice to do it outside of root subdirectories.You can check your partition in /proc/mounts or through df utilityTo go furtherWe covered formatting the filesystem through different format, partitioning it into logical volumes to then be mounted onto your file system, which handles different files types through inodes and binaries.There was a lot to uncover, and I was not expecting it to take this long, but I wanted to give a full picture of what file system are like, from the storage to its terminal.Each filesystem format has their own implementations, and could be interesting to check at a low-level. We could perform a write/read tests to check which ones are more performant.There is a lot of commands available to play with files in Linux, like sed/awk/grep/sort/cut, and they will be covered in a later article.Check out some links below to dive deeper into the subject. Credits https://linuxiac.com/linux-file-system-types-explained-which-one-should-you-use/ https://www.linux.com/training-tutorials/linux-filesystem-explained/ https://opensource.com/life/16/10/introduction-linux-filesystems https://tldp.org/LDP/intro-linux/html/sect_03_01.html https://en.wikipedia.org/wiki/Flash_memory https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9-beta" }, { "title": "Process Management", "url": "/maxime-lair/posts/process_management/", "categories": "RoadTo, OS Concepts", "tags": "linux, process, initd", "date": "2022-01-01 19:00:00 +0100", "snippet": "In this article, we will talk about process management in any IT operation, we will start by defining what process and managing them means, what commands can help us achieve this, and common pitfalls.My lab is running on a Centos 9 Stream, which came out this very month. Most of this will apply to Linux distribution, I will not cover Unix or Windows command.DefinitionA process is any active (i.e. running) instance of a program. A program is a sequence of instructions understandable by the CPU. It can be a ready-to-run program like an executable file.Process management will handle this process life cycle. From the time a process is born to when It is terminated, It can be runnable, running, sleeping (in memory or on disk) and zombie states.Process lifecycleInitAll processes are descendants of the init process, whose PID is one.You can check this init process existence with:Each process has a parent, and a list of children and sibling.When creating a new process, they can either be spawned from fork() or exec(). Difference is fork is an almost full duplication of the original process (new PID, etc.) while exec replaces the original process.For more informations, check out copy-on-write for fork optimization.Running stateIt is when It will start to use the most precious resource in the system: the CPU. But the CPU can not answer to all process at the same time, It has a limited pool. That’s when the scheduler comes in to sort it through.After being created, your process will wait until the CPU scheduler decides to let him run (i.e. execute his set of tasks).These states are : Running or runnable (R) - where process waits for the scheduler dispatch Uninterruptible sleep (D) - does not react to any signals and waits for the resources Interruptable Sleep (S) - will react to signals and the availability of resourcesStopped stateAfter your process is done with his set of tasks, It will naturally try to run the exit system call, which will releases its data structure and send a SIGCHLD signal to its parent process.The child process will stay in a ZOMBIE state until the parent use wait() system call to reap zombies.If the parent died in-between this, the child will re-attach to its grandparents, up until init process. At this point, init will periodically executes the wait system call to reap any zombies in its direct descendant.BonusIt’s possible to know at any given time the number of process by states and user:As you can notice, there is a lot of sleeping processes. You can also check out what are those substates, like Ss+ or Ssl.Helpful commandsChecking process stateProcess are the bread and butter of your operation, as they will always be the starting point of any resource comsumption.In an IT operation setting, we will have to monitor this number of process as It’s an interesting metric to have in your pocket.Let’s say you had multiple system failures over a few weeks, where disk comsumption was hard to predict as It was bursty and in bulk. You check out the number of process spawned/killed over time and notices a trend, It appears that when the number of process goes beyond a certain point, there was a 50% chance to encounter a system failure. It did not solve your problem, but It got you an investigation starting point, and can help you predict ressources comsumption in the future. For example, you can set an alert on this treshold because you know a system failure can happen in this context.There are multiple ways to check the state of a process in Linux.Command-line toolsTwo commands are mainly used to check out a process in Linux, they are named ps and topps means process snapshot, It will print you a report of the current processes. Be careful as snapshot/report always mean dealing with static data. It is nice to use if you want to get familiar with process management, but It is hardly used in production. It does not bring you context, and be falsely leading, as you are just playing roulette with the CPU scheduler.For example, you could think a process is stuck in ZOMBIE state, but you were just seeing it in between its SIGCHLD and exit calls.If you want to show a process tree:top is the same exact command as ps but It’s run periodically over time.There is many alternatives, using Python or nodeJS, to top, like htop, atop, nmon, vtop, bashtop, gtop.An example with bashtop:But as usual, It is only nice to get familiar with a specific system, but It is not helpful in the long run, or if you have to monitor hundreds of servers. Some of these alternatives also consumes a non-negligible amount of resources on the host as they pool informations from the entire system.Pseudo status fileIn Linux, everything is a file. It is also true for process, each time you spawn a new one with exec() or fork(), you create a new file, only available in memory, in the /proc/ filesystem.We can access our process state in this filesystem, along with other runtime informations on the system (like devices, diskstats, etc.)It can regarded as a control and information centre for the kernel, and quite a lot of system utilities (think ps or top) are simply parsing through this filesystem. A few details to remember is: All file size in /proc are 0 - they are only pointers to the kernel memory space Each numbered directories corresponds to an actual process ID - /proc/{PID}/You can know more about each file meaning hereThis is the best place to gather metrics, as you can directly access the information you need without any unecessary calls. If you need to often poll your system health, Its best to avoid any overhead.Process metricsNow that we understand what a process is, and how It evolves, we can start to lean into the management side. Question is: How do we effectively manage a process ? What set of metrics can we use to monitor a system health ?But first, what is a metric ? It’s a single, quantifiable type of data that is used for measurement and we use it to reach a measurable goal set by our project.We are not expert, and this is definitely a research theme. The main issue is : how do we avoid overfitting our dataset ? We are not data scientist, but we need to be careful as to not introduce to much noise in our metrics, we only want to scrape the main indicators.We will try to gather informations from the current best-practice. Let’s look at some of the most popular agent used to gather metrics from host: Node exporter Datadog Zabbix Nagios SematextFor most commercials solutions, they do not tell you what metrics they scrape, as It is their main selling point (less metrics, better performance).We can however see those main process metrics: Number of processes by user and name CPU and memory usage per process or binary namePolled in /proc:- /{PID}- /net- /sys- /mdstat- /schedstat- /vmstat- /meminfo_numa- /selfIn the case of datadog, they then can end up showing this type of visualization (% of CPU/log ratio):Note: There is a shifting focus on their containerization metrics, to be able to monitor those process metrics running inside a container.Take it a step furtherCheck out ressources on distributed and containerized infrastructures, as they make this more complex.You can also dive deeper into the subsystem of process management like signal handling, process/thread creation &amp;amp; termination and process scheduler. Creditshttps://www.redhat.com/sysadmin/linux-command-basics-7-commands-process-management https://www.baeldung.com/linux/top-command https://www.baeldung.com/linux/process-states https://www.kernel.org/doc/html/latest/filesystems/proc.html https://sre.google/sre-book/" }, { "title": "PFSense", "url": "/maxime-lair/posts/pfsense/", "categories": "ProjectBob, Infrastructure", "tags": "linux, pfsense, network", "date": "2021-12-28 19:00:00 +0100", "snippet": "First, It can help, to change keyboard layout in PFSense (will reset at next reboot)kbdcontrol -l /usr/share/syscons/keymaps/[YOUR LOCALE LANGUAGE].iso.kbdWAN/LAN configurationWAN is the wide area network, and is often considered the “outside” network. Internet is a type of WAN, but It could be your entire company network. It usually means “this is a network which has no finite geographical limit, systems could be anywhere in the world”.We want this WAN to be able to access the Internet in our case, and our main objective is to ping a popular DNS IP (like cloudfare 1.1.1.1 or google 8.8.8.8) and receives a response back. This means checking for ICMP filtering, and having a public IP.For this, we will be running this inside an hypervisor called ESXi, and with several public IPs provided by my service provider (I bought a whole block, registered to RIPE).First, let’s create our network bridge. We want our private network to be able to reach public network, but we don’t want random public network to poll our private network. That means they need to communicate through a firewall, that’s PFSense.First, our public network is defined with this IP block, this is what I was assigned to:Public IP: 141.95.187.105Gateway: 141.95.187.110Netmask: 255.255.255.248 (/29)We now have two choices: Create our PFSense, assign it this IP and create two VLANs that will act as WAN and LAN. Create vNIC together with vSwitch in our hypervisor ESXi that will act as our LAN network.Question is: do we create our LAN network at the hypervisor level, or VM level ? Both could work (see router on a stick)But It’s recommended to create them at the hypervisor level, as It makes it way easier to create new VMs in this LAN network, and in PFSense, you can restart one network adapter without bothering the other.So let’s create our vSwitch and vNic, check out this post to understand why Nic are neededWe create our vSwitch by indicating which uplink we want (physical network adapter on ESXi), I have two availables:One is used for the ESXi management (vmnic0), the other is free to use, so we create our vSwitch named vSwitchLab:Then we create a port groups, that will be standing behind this vSwitch, we call it Lab Networkand we create our vNic to stand on the other side:You might notice It created an IP automatically : 192.168.1.100 - It is a private networkAnd now, we are ready to create our PFSense VM, with two network adapters : One for our WAN One for our LANOne detail to note, we need to assign a MAC address on our outside-facing network card, as It will be facing the Internet. This MAC address will be linked with our public IP.On this first network adapter, we use “Advanced…” and assign its MAC address. One detail is If we create VLAN from this network adapter, they will all share the same MAC address.Now we can start our PFSense, and after installation, we are greeted by this menu:We want to first assign interfaces, and then set our interface(s) IP address.When assigning interfaces, we want: em0 (VM Network) to be our WAN em1 (Lab Network) to be our LANDo not create any VLAN or downgrade the console to http, It is not needed.Then we set our IPs, they are static on both sides, since we do not have a DHCP server assigning them: em0 &amp;lt;-&amp;gt; WAN &amp;lt;-&amp;gt; Public IP: 141.95.187.105/29 with gateway 141.95.187.110 em1 &amp;lt;-&amp;gt; LAN &amp;lt;-&amp;gt; Private IP: 10.0.0.1/24 (could have used up to /8) with no gateway (as we are our own gateway)I could have used any IP in available private subnets : 10.0.0.0/8, 172.16.0.0/12 or 192.168.0.0/16Let’s try to ping an outside of network DNS:Our firewall is able to ping outside network, all is left is to create a VM with a GUI in the LAN network to reach PFSense web interface, and have it able to reach outside network by going through our PFSense firewall.Installing a lab in our LANNetwork connectivityReasoningNote: I will be using a CentOS 9 Stream, which came out this month.When creating the VM in ESXi, create a network adapter assigned to “Lab Network”After installing our OS, our system is unable to connect to its network. This is normal as he doesn’t have a DHCP server to give him his IP, we have to manually set it up.For this, I will be using NetworkManager CLI, also named nmcli, as It’s better suited for scripting and I want to test it out. Others setups will probably prefer using configuration files, and that would have been my first choice had I not decided to try out this CLI.My end goal would be to include the next commands in my Ansible playbook when I want to create more VMs.Set up static IP through nmcliFirst, we check which devices we will use:ens32 will be our dedicated device. Here nmcli dev stands for nmcli device status - It’s a nice shortcut.It’s possible to create this device through nmcli but we already did that when creating the VM.Our network, as defined in PFSense, will look like this:Network: 10.0.0.0/24Gateway: 10.0.0.1Broadcast: 10.0.0.255Mask(/24): 255.255.255.0Since this is our first system we set up in this network, let’s just increment and use the first available IP: 10.0.0.2For others installations, we will try to have a DHCP server to assign this automatically.nmcli connection modify ens32 ipv4.addresses &quot;10.0.0.2/24&quot; ipv4.gateway &quot;10.0.0.1 ipv4.method &quot;manual&quot; ipv4.dns &quot;10.0.0.1&quot;Our device is now set with his static IP, and indicating the gateway creates the route. Be careful as the default connection is usually with DHCP activated (ipv4.method set to manual instead of auto)If you need to check all availables variables, check out your configuration in /etc/NetworkManager/system-connections/[CON-NAME].nmconnectionAll green, all good ! Let’s try to ping our gateway:Naturally, It works, as we are in the same LAN as defined on ESXi and our VM configuration. But can we reach outside network ?We can, great ! If that didn’t work, we could have looked into the ICMP traffic, if PFSense was rejecting them by default.Now we can connect to PFSense GUI and start enhancing our setup from our CentOS in LANBy default, this GUI is only available on the LAN side, as to not expose it to outside network (how easy would It be to scan IPs for PFSense first install).Default user is admin / pfsenseAnd we are good to go !All is left is to add some firewall rules, create new VMs in this network, and we are all set !We end up with this infrastructure:Lanlab(s) will be used to host kubernetes nodes, and esxi-lab is here for quick services testing.For next hosts, we can rename them withhostnamectl set-hostname lanlabXThen reboot.If you can’t resolve DNS entry (you can wget http://1.1.1.1 but not wget http://google.com), check your configuration in /etc/resolv.conf ; I had to reboot on mine, even though nmcli worked on others hosts..Configuring PFSenseNow that we have a working traffic network, we need to set up some policies.Let’s use PFSense GUI -&amp;gt; System -&amp;gt; Setup WizardWe don’t have anything to change, besides maybe your DNS server, just make sure on step 4/9 to select Static and not DHCP.Next, we want to set up SSH tunneling, so we can SSH from a remote terminal into our lan-lab networks.In System -&amp;gt; Advanced -&amp;gt; Check Enables ssh-agent forwarding support - This will allow us to authenticate with our local RSA key when tunneling instead of PFSense’s one.Don’t forget to remove Password authentication in the long-run and only allow public key authentication.Now we need to authorize some ports from WAN into our LAN. You can check out some common skeleton, but It really depends on your infrastructure. As for mine, I’m still modifying it heavily.For now, we will just authorize SSH connections in our network.SSH TunnelingWe want to connect from a remote terminal in our lanlab systems, we will try to connect through a SSH tunnel by bouncing off PFSense into our Lab network.We will be using our ~/.ssh/rsa_pub.key from our local terminal.We go back on PFSense, and on the GUI (User Manager -&amp;gt; Edit -&amp;gt; Authorized SSH Keys), we add our key (you could also add it through the terminal).Since our plan is : (Local Terminal) –&amp;gt; (PFSense) –&amp;gt; (Remote lab), we will use ssh -J USER@PFSENSE USER@LABOf course we could use a ProxyCommand file to avoid using this jump everytime. Let’s disable password authentication on PFSense after we have spread our id_rsa.pubAfter adding a DNS entry for PFSense (too lazy to remember the IP), I have this configuration for accessing my LAB:I can’t use DNS entries for the systems inside the LAB, as PFSense can’t make the link between it yet (He doesn’t have any DNS or DHCP server with those entries yet).Now, we are ready to connect into our infrastructure:This should not be our final setup, as we will want to use OpenVPN at one point instead of this tunnel, and some DHCP/DNS, but It’s working well for now. There is still much more to do with setting up a DEV/UAT/PROD environment. Credits https://docs.ovh.com/au/en/dedicated/pfSense-bridging/#requirements https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-configuring_ip_networking_with_nmcli" }, { "title": "Tmux", "url": "/maxime-lair/posts/tmux/", "categories": "ProjectBob, QoL", "tags": "linux, tmux, bash", "date": "2021-12-27 19:00:00 +0100", "snippet": "Tmux funAs my number of services piles up, I’m starting to repeat the same step of connecting to my VPS and on different user every day.I want to try and automate that, so I can create multiple sessions on the different servers I haveFor now I’m only on a single host, but I have 3 services (httpd, proxy and monitoring) to handle, and I sometimes want to switch back to a sudo user so I can restart docker daemonLet’s use Tmux to handle the different sessions, and tmuxinator to template it !Tmux cheatsheet Tmux is a terminal multiplexer an alternative to GNU Screen . In other words, it means that you can start a Tmux session and then open multiple windows inside that session. Each window occupies the entire screen and can be split into rectangular panes.With Tmux you can easily switch between multiple programs in one terminal, detach them and reattach them to a different terminal.Tmux sessions are persistent, which means that programs running in Tmux will continue to run even if you get disconnected. All commands in Tmux start with a prefix, which by default is ctrl+b.The main commands to remember are detaching session (how to pause tmux), switching to next window or pane:ctrl + b + d detach sessionctrl + b + n next windowctrl + b + arrow_key next paneFor more, check out the cheatsheetLet’s create (or add) to ~/.tmux.conf the ability to scroll up/down on our pane and have some nice terminal olors:$ cat ~/.tmux.confset -g mouse onset -g default-terminal &quot;screen-256color&quot;TmuxinatorSince we will be repeating the same command to create session/window/pane, let’s use a template script called tmuxinatorAfter using it, I really like the configuration file, but I hate installation/documentation parts. It’s messy.Don’t forget to set up $EDITOR variable in your .bashrcexport EDITOR=&#39;vim&#39;Template for binsh.ioLet’s use it on a current project I’m working, hosted on a VPS on OVHI want to connect through SSH with 2 differents users: produser - applicative user, running several docker containers centos - default sudo user, running docker daemonLet’s create 3 panes on produser (one for each service), and 2 on centosOur template file looks like this:$ tmuxinator new VPS_OVH$ tmuxinator open VPS_OVH$ cat .config/tmuxinator/VPS_OVH.yml | grep -v &quot;#&quot;name: VPS_OVHroot: ~/on_project_exit: tmux kill-session -t VPS_OVHwindows: - produser: layout: main-vertical panes: - produser_1: - ssh produser@vps.ovh - clear - produser_2: - ssh produser@vps.ovh - clear - produser_3: - ssh produser@vps.ovh - clear - centos: layout: even-vertical panes: - centos_1: - ssh centos@vps.ovh - clear - centos_2: - ssh centos@vps.ovh - clearA few things to consider: When we detach from the session, we kill it, we do this to ensure we dont forget and create dozens of session. I don’t see any use case for disabling this. We connect to each session and clear the terminal, to not have any text remnants from our local terminal The host target is a DNS entry I set in /etc/hosts with the VPS IPWe could easily automatically generate this type of template for more VPS, for an entire subnet, etc.Then, we add an alias to start the tmuxinator project, as we dont want the hassle of remembering the exact commandIn our .bashrcalias vps-ovh=&#39;tmuxinator start VPS_OVH&#39;And now we can just connect to our VPS by typing the commandkali@kali:~$ tmux lsno server running on /tmp/tmux-1000/defaultkali@kali:~$ vps-ovh[detached (from session VPS_OVH)]kali@kali:~$ tmux lsno server running on /tmp/tmux-1000/defaultHere we notice how there is no tmux session before the command, and none remaining after detaching from it. It works as intended, nice.And our tmux result:We can switch easily to the centos user by typing ctrl + b + nAll wellIn the future, we might want to add more hosts / setups when we start getting more hosts. Maybe even add an automatic host fetch list" }, { "title": "Docker introduction", "url": "/maxime-lair/posts/docker_introduction/", "categories": "ProjectBob, Infrastructure", "tags": "linux, docker, grafana, traefik, prometheus, httpd, node_exporter, cadvisor", "date": "2021-12-26 19:00:00 +0100", "snippet": "This article is the first introduction to docker, and how to run containers on a single host.The end-goal is to have an infrastructure running several services: Setup docker and docker-compose Web application powered by httpd Traefik to handle routing and load-balancing Prometheus &amp;amp; Grafana for monitoring purpose Node_exporter (for host metrics) and Cadvisor (for docker metrics) on each containerAll these applications will be running inside dockers, as to be scalable in the futureIt will start as a single host project and will slowly be turning into multiple-hostsDockerWe need to install docker on each host, It has to be done through a sudo or root user as the final process will be running with root privileges (constraint of docker).We follow the instructions as per recommended to guide us throughWe start off with the first docker installationPre-requisitesFirst we check if docker is already installed, we do not want to install on top of an already available version.Then, we add docker repository as It is not available natively on Centosdocker sudo yum install -y yum-utilssudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoNote Since we are running it on Centos 8, It would have been nice to have used dnf instead of yum as the latter is getting deprecated.Install docker engineDocker requires several tools to work, named docker engine and containerdsudo yum install docker-ce docker-ce-cli containerd.ioIt takes around 1m30 to complete.Test dockerNow we can start the docker engine and run a simple hello-world$ sudo systemctl start docker  $ $ sudo docker run hello-world Unable to find image &#39;hello-world:latest&#39; locallylatest: Pulling from library/hello-world2db29710123e: Pull complete Digest: sha256:cc15c5b292d8525effc0f89cb299f1804f3a725c8d05e158653a563f15e4f685Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashWe check a few commands to familiarize ourselves:$ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest feb5d9fea6a5 2 months ago 13.3kB$ sudo docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESThen we stop the service previously started$ sudo systemctl stop docker Warning: Stopping docker.service, but it can still be activated by: docker.socket$ sudo systemctl stop docker.socketNote It seems when starting docker, you start two units: service and socket. You have to manually stop them both as one could wake the other up if any request comes in. I have no idea why It is done this way, start and stop should be a one-liner command.We stop the service because we might have more settings to adjust, and this is not the proper way to activate it anyway in the long run (we want to use systemctl enable to have it available at start-up).Post-install shenaniganThe Docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The Docker daemon always runs as the root user.We do not want to preface the docker command with sudo, so we create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group.Dedicated user creationWe will create a user named produser for running docker, I simply added it with useradd and provided it with a password.The group docker is created by default now, so we just have to add the new user to the group$ sudo usermod -aG docker produserTest the new userNow all we need to do is test creating a small docker container with this new userWe connect as produser$ su produserAnd we test a simple hello-world$ docker run hello-worldIt won’t work as we previously stopped the docker daemonWe connect back on our sudo user and we enable the service (so It’s available at next reboot) and start it$ sudo systemctl enable docker.service$ sudo systemctl start docker.service$ sudo systemctl enable containerd.service$ sudo systemctl start containerd.serviceThen we connect back on our new user, and test its status$ systemctl status containerd.service● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) Docs: https://containerd.io Main PID: 84811 (containerd)It’s running well, we can try our hello-world$ docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.It works, we can move on onto the next part, starting each docker container containing our different service (web, monitoring, proxy)Httpd web serverWe want to deliver web pages through our server by hosting a docker container running a httpd processWe could use an alpine docker image, and add the web server onto it, but there is already an image available on the official website: https://hub.docker.com/_/httpdTo spice it up, we will try to scale the number of web servers, each running the same image, they will deliver the web server on the host on differents ports ranging from 35000 to 35100.My objective at the end is to have redundancy, if the first one fails, we can always send them onto the second.The web content we will deliver is the one available at https://github.com/maxime-lair/binshTest the docker imageFirst, let’s test the image provided. We do this to ensure any issues down-the-line would come from our HTML pages or docker configurationWe create a new repository, where we will work from, and ultimately have our web pages and docker configuration available at:$ mkdir httpd-service$ cd httpd-serviceThen we can test our docker image:$ docker run -dit --name my-apache-app -p 8080:80 -v &quot;$PWD&quot;:/usr/local/apache2/htdocs/ httpd:2.4Unable to find image &#39;httpd:2.4&#39; locally2.4: Pulling from library/httpde5ae68f74026: Pull complete bc36ee1127ec: Pull complete 0e5b7b813c8c: Pull complete a343142ddd8a: Pull complete 94c13707a187: Pull complete Digest: sha256:0c8dd1d9f90f0da8a29a25dcc092aed76b09a1c9e5e6e93c8db3903c8ce6ef29Status: Downloaded newer image for httpd:2.4478222b0c467cb72d244761c55a327750dd8edb5bb0bb33e9ac6972356ad4fe6It started running, can we check the process and web page ?$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES478222b0c467 httpd:2.4 &quot;httpd-foreground&quot; About a minute ago Up About a minute 0.0.0.0:35000-&amp;gt;80/tcp, :::35000-&amp;gt;80/tcp my-apache-app$ curl http://localhost:35000&amp;lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&amp;gt;&amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Index of /&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Index of /&amp;lt;/h1&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;The image is running well, able to deliver our web pages onto the host, let’s stop it and write our docker files now !$ docker stop 478222b0c467478222b0c467$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESDocker composeSince we want to start two web servers, let’s use docker compose instead of two separate docker filesIn order to install it, we need to switch back to our sudo user$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composeWe add execution rights on the binary$ sudo chmod +x /usr/local/bin/docker-composeThen we test it by switching back to produser$ su produser$ docker-compose --versiondocker-compose version 1.29.2, build 5becea4cNote We will not use any Dockerfile as we do not need to edit the image configuration.Now we edit our docker-compose.yml file$ cat docker-compose.ymlversion: &quot;3.9&quot;services: web: ports: - &quot;35000-35100:80&quot; image: &quot;httpd:2.4&quot; volumes: - ./binsh/:/usr/local/apache2/htdocs/ networks: - webzonenetworks: webzone: driver: bridgeWe indicate one service called web running on range 35000-35100 on the host and mapped to default httpd port (80)Then we indicate the HTML files we want to use, and where to place them on the containerWe could do it through a Dockerfile, but the advantage of using volumes is the live update, if we modify any files on /binsh/ on the host, we do not need to restart any docker container, as the changes will be effective immediately. If we did a COPY through a dockerfile, this would not be the case.Now we retrieve the web files we want to host:$ git clone git@github.com:maxime-lair/binsh.gitAdvantage is : If I want to update my webfiles, I can leave the container running, and just re-do a git clone to have it available live.Now we can start the docker-compose:$ docker-compose up -d --scale web=10Creating network &quot;httpd-service_webzone&quot; with driver &quot;bridge&quot;WARNING: The &quot;web&quot; service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash.Creating httpd-service_web_1 ... doneCreating httpd-service_web_2 ... doneCreating httpd-service_web_3 ... doneCreating httpd-service_web_4 ... doneCreating httpd-service_web_5 ... doneCreating httpd-service_web_6 ... doneCreating httpd-service_web_7 ... doneCreating httpd-service_web_8 ... doneCreating httpd-service_web_9 ... doneCreating httpd-service_web_10 ... doneWe check on the host if we see our ports opened:$ netstat -tlpn (Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:35025 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35026 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35027 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35028 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35029 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35030 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35031 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35032 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35033 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:35034 0.0.0.0:* LISTEN -Sure enough, our web servers are up and running, able to deliver the web pages on port 35xxxIf you check httpd logs, we notice some warnings at the web server start-up because we did not change the ServerName in the httpd configuration, as the process is unsure of the correct IP It is being hosted on. When we get our proxy running, we will be able to update this to our FQDN binsh.ioIf you want to start your container in the background, you can add -d argument on the start$ docker-compose up -d Starting httpd-service_web ... doneBut as for the docker warning, we now have an issue on our hands: how to load-balance any requests made to binsh.io on port 80 to the different web servers available on port 35xxx ? We could use Nginx, but we would have to manually update its configuration everytime we remove or add a container.Let’s try to use a new proxy solution called TraefikTraefikAdding load balancer and reverse-proxyFrom its website Traefik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy.Traefik integrates with your existing infrastructure components (Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS, …) and configures itself automatically and dynamically.Traefik upgraded from V1 to V2 last year, and It’s a bit difficult to understand how It now all works with services, endpoints, middlewares, etc.Let’s try it nonethelessFirst, we create two configurations for traefik, a static configuration, which tells him what do we expose (where do we want requests to come in)traefik.yml ## traefik.ymlentryPoints: http: address: &quot;:80&quot; https: address: &quot;:443&quot;# Docker configuration backendproviders: file: filename: dynamic_conf.yml watch: true docker: endpoint: &quot;unix:///var/run/docker.sock&quot; exposedByDefault: false# API and dashboard configurationapi: insecure: trueWe ask to listen on port 80 for http and https for 443, the usual.Then we provide him a dynamic configuration file, that we will explain later, we want to keep watching this file in case of update so It can be applied live.Then we tell him to listen for any new containers on the host, as we want to monitor/route onto themLastly, we expose the API, in an insecure way for now, we will add TLS/HTTPS laterWe create our dynamic_conf.yml filehttp: routers: http_router: rule: &quot;Host(`binsh.io`)&quot; service: web services: web: loadBalancer: servers: - url: &quot;http://httpd/Here we provide a simple rule for routing: if we receive requests for our FQDN binsh.io, we load balance it on any servers responding to httpdWhat is this httpd url then ? Since they will be running on the same docker-compose, they will share the same default network. In this case, traefik container will be able to call up any httpd container by this url.The docker-compose.yml file:version: &quot;3.9&quot;services: httpd: ports: - &quot;35000-35100:80&quot; image: &quot;httpd:2.4&quot; volumes: - ./binsh/:/usr/local/apache2/htdocs/ - ./my-httpd.conf:/usr/local/apache2/conf/httpd.conf networks: - webzone traefik: image: traefik:latest command: --api.insecure=true --providers.docker ports: # The HTTP port - &quot;80:80&quot; # The Web UI (enabled by --api.insecure=true) - &quot;8080:8080&quot; volumes: # So that Traefik can listen to the Docker events - /var/run/docker.sock:/var/run/docker.sock - $PWD/traefik.yml:/etc/traefik/traefik.yml - $PWD/dynamic_conf.yml:/dynamic_conf.yml networks: - webzone - proxynetworks: webzone: driver: bridge internal: true proxy: driver: bridgeA few changes to be noted: We added networks, so we can avoid exposing our dozens of possible web servers ports on the host, they will be in their own, secluded network webzone. Only traefik will be able to serve as bridge to the outside (to the host and beyond). We added traefik service, so It can run together with the httpd server. We added httpd configuration to change the servername to our FQDNNow, we can start and scale our web servers depending on our needs$ docker-compose up --scale httpd=3 -dWARNING: The &quot;httpd&quot; service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash.Starting httpd-service_httpd_1 ... doneStarting httpd-service_httpd_2 ... doneStarting httpd-service_httpd_3 ... doneStarting httpd-service_traefik_1 ... doneAnd we are now able to access our website from outsideEach GET is load balanced on the 3 different containers we just spawned.Only the needed ports are opened on the host$ netstat -tlpnActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - We also have access to traefik dashboard on port 8080Adding TLS and HTTPSNow that we have access on port 80, let’s try to redirect on port 443 and handles everything more securelyWe create a file hosting our TLS certificates, and we need to review its file right (or we will get a slap on the wrist later when we start traefik)$ touch acme.json$ chmod 600 acme.jsonThis file will hold the TLS certificates created by Let’s encryptFirst, we will secure our docker-compose.yml by activating the secure api, removing the dashboard port, and sharing the acme.json fileWe also added port 443 for HTTPS, since we will be redirecting all our traffic onto it. We want to be HSTS approved !$ cat docker-compose.yml version: &quot;3.9&quot;services: httpd: ports: - &quot;35000-35100:80&quot; image: &quot;httpd:2.4&quot; volumes: - ./binsh/:/usr/local/apache2/htdocs/ - ./my-httpd.conf:/usr/local/apache2/conf/httpd.conf networks: - webzone traefik: image: traefik:latest restart: unless-stopped command: --api --providers.docker ports: # The HTTP port - &quot;80:80&quot; - &quot;443:443&quot; volumes: # So that Traefik can listen to the Docker events - /var/run/docker.sock:/var/run/docker.sock - $PWD/traefik.yml:/etc/traefik/traefik.yml - $PWD/dynamic_conf.yml:/dynamic_conf.yml - $PWD/acme.json:/acme.json networks: - webzone - proxynetworks: webzone: driver: bridge internal: true proxy: driver: bridgeThen we move onto our traefik.yml file, we want 3 main points added: Redirect all HTTP trafic (port 80) to HTTPS (port 443) Secure the API (some kind of authentication required to access it) Add a certificate resolver - this will handled by Let’s encrypt for validating TLS certificate (not expired, right domain, can be trusted, etc.)$ cat traefik.yml ## traefik.ymlentryPoints: http: address: &quot;:80&quot; http: redirections: entryPoint: to: https https: address: &quot;:443&quot;# Docker configuration backendproviders: file: filename: dynamic_conf.yml watch: true docker: endpoint: &quot;unix:///var/run/docker.sock&quot; exposedByDefault: false# API and dashboard configurationapi: dashboard: truecertificatesResolvers: letsencrypt: acme: email: &amp;lt;DEDICATED EMAIL ADDRESS&amp;gt;@gmail.com storage: acme.json httpChallenge: entryPoint: httpNote You have to create a mail address to receive importants messages (expiration, etc.) when activating TLS, see. For now this address is external, but we would want to have a self-hosted mail server with MX/SPF/DKIM/DMARC protectionNow, onto the interesting part, we have multiple things to do: Authenticate user trying to connect to Traefik API Route users onto the dashboard or onto the web server depending on their requests (since we removed port 8080) Make sure they are using HTTPSThis is all done through routers and middlewares on Traefik, routers will catch requests depending on their rule to then direct them onto a list of middlewares (which can authenticate, redirect, add headers, etc.), and if they managed to get through, route them onto a serviceThe logic will always be in Traefik-v2 : EntryPoints -&amp;gt; routers -&amp;gt; middlewares -&amp;gt; services -&amp;gt; providersFor authentication, we will use BasicAuth with user:pass logic, although we could have used any external authentication services (AWS, etc.)Note I had some trouble when generating them on my Centos8 with htpasswd as they were not recognized in Traefik. I had to use an online generator.You can use this command to generate your own user/password$ echo $(htpasswd -nb xkcd &amp;lt;VERY SECURE PASSWORD&amp;gt;) | sed -e s/\\\\$/\\\\$\\\\$/gxkcd:$$apr1$$&amp;lt;VERY SECURE HASH&amp;gt;Now we can write our dynamic_conf.yml$ cat dynamic_conf.yml http: routers: http_router: rule: &quot;Host(`binsh.io`)&quot; service: web middlewares: - traefik-https-redirect tls: certResolver: letsencrypt traefik_router: entrypoints: http rule: &quot;Host(`traefik.binsh.io`)&quot; service: api@internal middlewares: - traefik-https-redirect traefik_secure_router: entrypoints: https rule: &quot;Host(`traefik.binsh.io`)&quot; middlewares: - traefik-auth tls: certResolver: letsencrypt service: api@internal services: web: loadBalancer: servers: - url: &quot;http://httpd/&quot; middlewares: traefik-auth: basicAuth: users: - &quot;&amp;lt;USER&amp;gt;:$apr1$&amp;lt;PASS HASH&amp;gt;&quot; traefik-https-redirect: redirectScheme: scheme: https permanent: trueWe use a subdomain to redirect users going for web content (top domain binsh.io) and the ones going for Traefik dashboard (traefik.binsh.io)All traffic is now forced into HTTPS with a valid certificate (which will be automatically renewed by Traefik), and we only force an authentication for users going onto Traefik dashboardWe can now start our docker-compose to finish adding HTTPS and auth to our infrastructure$ docker-compose up -d --scale httpd=3Starting httpd-service_traefik_1 ... Starting httpd-service_traefik_1 ... doneStarting httpd-service_httpd_1 ... doneCreating httpd-service_httpd_2 ... doneCreating httpd-service_httpd_3 ... doneHTTPS on binsh.io:And on traefik.binsh.io (after basicAuth):Now that we added our proxy, we are able to have HTTPS content on our docker container, our infrastructure could be running live. But we need a way to monitor this host. Imagine we run into a disk failure, we need to have metrics on how It happened, and be alerted when It happens (or before even).Prometheus and GrafanaWe want to install Prometheus and Grafana, they will be our monitoring stronghold to retrieve and analyze any collected metrics.We will not bother with high-availability/fault-tolerant issues for now, this is a subject on which I’m not finding any design patterns. Only answer I got was “Use sharding and only one prometheus server per datacenter”, but It doesn’t cover any case on when the prometheus server has an issue (hardware failure or else).Prometheus installation Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud in 2012. Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.Why use time series collection ?Typically, metrics are best used for monitoring, profiling, and alerting. The efficiency of summarizing data makes them great for monitoring and performance profiling because you can economically store long retention periods of data, giving you dashboards that look back over time. They are also great for alerting because they are fast and can trigger notifications almost instantaneously without the need for expensive queries. Metrics represent roughly 90% of the monitoring workload. sourceLater on, we will think on adding a log-based monitoring (like Filebeat or else) and trace-based monitoring (like Jaeger).Again, we have a provided prometheus docker that we can use out-of-the-box: https://docs.docker.com/config/daemon/prometheus/ https://hub.docker.com/r/prom/prometheusOur endgoal is to have a prometheus service available which we can access on prometheus.binsh.ioWe create a prometheus.yml which will be the configuration, It should be static across all prometheus docker since It will define scrape_interval and targets$ cat prometheus.yml # my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: &#39;binsh-monitor&#39;# Load rules once and periodically evaluate them according to the global &#39;evaluation_interval&#39;.rule_files: # - &quot;first.rules&quot; # - &quot;second.rules&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&#39;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config. - job_name: &#39;prometheus&#39; # metrics_path defaults to &#39;/metrics&#39; # scheme defaults to &#39;http&#39;. static_configs: - targets: [&#39;localhost:9090&#39;] - job_name: &#39;docker&#39; # metrics_path defaults to &#39;/metrics&#39; # scheme defaults to &#39;http&#39;. static_configs: - targets: [&#39;172.17.0.1:9323&#39;]Note Experimental test: docker daemon can now export metrics onto PrometheusThe IP for the docker job is the docker0 interface$ ip addr show docker03: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:2f:68:96:ac brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0Funny thing is : the documentation recommends using localhost, but we can’t access those metrics from the loopback device when running prometheus inside docker. Even if we try to expose the port on the host, It does not work.You have to amend (or create if not existing) /etc/docker/daemon.json and add$ cat /etc/docker/daemon.json{ &quot;metrics-addr&quot; : &quot;0.0.0.0:9323&quot;, &quot;experimental&quot; : true}We have to bind it on 0.0.0.0 (any addr) because prometheus might need to access it from outside, but this could be a security issue and we will tackle it later.Then we have to restart the docker service (be careful of shutting down any docker running)$ sudo systemctl daemon-reload $ sudo systemctl restart dockerWe will later on check on the differences between cadvisor metrics and the one retrieved from the daemon itself.For now, we only bother about monitoring our single host with a static configuration. Later on, when we scale to multiple hosts, we will have to dynamically update the host list to search for them. It can be done with DNS, AWS, IP, file based configuration.Grafana installation Grafana open source is open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics, logs, and traces no matter where they are stored. It provides you with tools to turn your time-series database (TSDB) data into insightful graphs and visualizations.The project has since then branched into multiples components, with Grafana dashboard (the one we are using), Loki (Prometheus but for logs) and Tempo (for correlation)Grafana docker: https://grafana.com/docs/grafana/latest/installation/docker/ https://hub.docker.com/r/grafana/grafanaOur endgoal is to have a grafana service available at grafana.binsh.io and able to reach prometheus serviceConfiguration-wise, we want to amend a few parameters,Normally, you could retrieve a base file with this type of command:docker run --rm grafana/grafana-oss cat /etc/grafana/grafana.ini &amp;gt; base-custom.iniBut the way this image was made is .. with never-ending script waiting for logs, so the container can never stop itself with –rm argumentSo we have to improvize by creating the container, executing a command inside it, then shutting it down, use it at your own risk:docker run -d grafana/grafana | xargs -I % sh -c &#39;docker exec % cat /etc/grafana/grafana.ini &amp;gt; base-grafana.ini; docker stop %&#39;It creates a new file base-grafana.ini with the default grafana configuration.The file is 1k lines long, but everything has defaults so we will just leave what’s important to us in case we need to amend them.Then we create our target grafana.ini that will be used on the grafana container spawned by docker-compose$ cat grafana.ini################################### General ###################################app_mode = productioninstance_name = grafana.binsh.io#################################### Paths ####################################[paths]# Directory where grafana can store logs;logs = /var/log/grafana# Directory where grafana will automatically scan and look for plugins;plugins = /var/lib/grafana/plugins# folder that contains provisioning config files that grafana will apply on startup and while running.;provisioning = conf/provisioning#################################### Server ####################################[server]# The http port to use;http_port = 3000#################################### Analytics ####################################[analytics]reporting_enabled = falsecheck_for_updates = false#################################### Security ####################################[security]disable_initial_admin_creation = trueadmin_user = &amp;lt;VERY SECURE USER&amp;gt;admin_password = &amp;lt;VERY SECURE PASSWORD&amp;gt;#################################### Users ###############################[users]allow_sign_up = false#################################### Basic Auth ##########################[auth.basic];enabled = trueDocker-compose fileNow that we have our grafana and prometheus configurations, we can create our docker-compose file$ cat docker-compose.ymlversion: &quot;3.9&quot;services: prometheus: ports: - &quot;36200:9090&quot; image: &quot;prom/prometheus&quot; volumes: - prometheus-storage:/prometheus - $PWD/prometheus.yml:/etc/prometheus/prometheus.yml networks: - proxy grafana: ports: - &quot;36300:3000&quot; image: &quot;grafana/grafana-oss&quot; volumes: - grafana-storage:/var/lib/grafana - $PWD/grafana.ini:/etc/grafana/grafana.ini depends_on: - &quot;prometheus&quot; networks: - proxynetworks: proxy: driver: bridgevolumes: grafana-storage: prometheus-storage:For now we will let them on the host network, later on we will have them behind Traefik in their own secluded network.One part stands out : We added two volumes so we do not lose any data in case any container crash On grafana It saves us any dashboard/user we created On prometheus It saves all metrics retrieved from the host, but I think prometheus keeps anything under 2 hours in memory and only writes it on disk If it is any older, so we would lose any recent data (We will need data replication to avoid this)Also we wait on prometheus to start for spinning up Grafana, as the dashboard would be useless without any data to pull from.Services configurationNow all we need to do is check our service health, for now they are not behind any proxy, so we can access them on their port directly (ie. :36200/36300)Let’s check Prometheus, are our targets defined in our configuration able to spit up metrics?Now can our Grafana retrieve those Prometheus metrics ?We connect to grafana and add our datasource:And It works ! We have our (already provided) dashboard with metrics:We could create our own dashboard and even our own metrics once we get up and running, but for now we will just use already-made dashboard.Networks and routingNow that prometheus and grafana are running well in their own containers, we can move onto securing the stackLet’s start by questionning ourselves on what we want to expose.We currently have this setup:Our end-goal is to have something cleaner like this:So we will need to perform these two steps: Separate Traefik into its own docker-compose and add rules for docker/prometheus routing Put prometheus/Grafana in their own network spaceSeparate TraefikFirst, let’s split our current docker-compose for httpd-service.Currently, we have:cat docker-compose.ymlversion: &quot;3.9&quot;services: httpd: ports: 35000-35100:80 image: &quot;httpd:2.4&quot; volumes: - ./binsh/:/usr/local/apache2/htdocs/ - ./my-httpd.conf:/usr/local/apache2/conf/httpd.conf networks: - webzone traefik: image: traefik:latest restart: unless-stopped command: --api --providers.docker ports: # The HTTP port - &quot;80:80&quot; - &quot;443:443&quot; volumes: # So that Traefik can listen to the Docker events - /var/run/docker.sock:/var/run/docker.sock - $PWD/traefik.yml:/etc/traefik/traefik.yml - $PWD/dynamic_conf.yml:/dynamic_conf.yml - $PWD/acme.json:/acme.json networks: - webzone - proxynetworks: webzone: driver: bridge internal: true proxy: driver: bridgeFirst, we dont have to set ports nor networks on httpd container, It wasn’t necessary as traefik and httpd are sharing the same network (http-service_default), we can remove it.From docker-compose docs on networks: By default Compose sets up a single network for your app. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.Then we can remove the entire traefik part, It will be moved to its own docker-compose file in another directoryLet’s create this directory before we proceed any further$ mkdir traefik-service$ tree ..├── httpd-service│   ├── acme.json│   ├── binsh│   │   ├── index.html│   ├── docker-compose.yml│   ├── dynamic_conf.yml│   ├── my-httpd.conf│   └── traefik.yml├── monitoring-service│   ├── base-grafana.ini│   ├── docker-compose.yml│   ├── grafana_data│   ├── grafana.ini│   ├── prometheus_data│   └── prometheus.yml└── traefik-serviceWe simply copy docker-compose.yml and traefik related files located in httpd-service into traefik-service (so we will not have to retype everything once we get there)$ cp httpd-service/docker-compose.yml traefik-service $ cp httpd-service/dynamic_conf.yml traefik-service $ cp httpd-service/traefik.yml traefik-service $ tree traefik-service traefik-service├── docker-compose.yml├── dynamic_conf.yml└── traefik.yml└── acme.jsonNow we can modify our docker-compose in httpd-service directory$ cat docker-compose.ymlversion: &quot;3.9&quot;services: httpd: image: &quot;httpd:2.4&quot; volumes: - ./binsh/:/usr/local/apache2/htdocs/ - ./my-httpd.conf:/usr/local/apache2/conf/httpd.conf networks: - traefik - defaultnetworks: traefik: external: true name: traefik-service_defaultHere we only have our httpd server running. We could use a single command line instead of using docker-compose, but if we ever need to add a database or more, we can easily add it.One inconvenience is we have to join Traefik network which will be started through another docker-compose, and we run into “chicken or egg” issue. We can’t start our docker-compose unless traefik network is up, this is usually fine as we can’t access the server anyway if our proxy is not up, but this is still a hard dependency which will require schedulingWe could have made Traefik join httpd network instead, but If we think about it, scaling-wise, this is a bad idea. We do not want to amend Traefik docker-compose file everytime we will add or remove a service like httpd (as It will have to update its network every time).Now let’s set up traefik-service (that we previously copied files onto)We only have to amend our docker-compose file by removing the httpd:$ cat docker-compose.yml version: &quot;3.9&quot;services: traefik: image: traefik:latest restart: unless-stopped command: --api --providers.docker ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: # So that Traefik can listen to the Docker events - /var/run/docker.sock:/var/run/docker.sock - $PWD/traefik.yml:/etc/traefik/traefik.yml - $PWD/dynamic_conf.yml:/dynamic_conf.yml - $PWD/acme.json:/acme.jsonLet’s clean-up any networks remnants and start traefik$ docker network prune$ docker-compose up -d Creating network &quot;traefik-service_default&quot; with the default driverCreating traefik-service_traefik_1 ... doneThen we start our httpd containers$ docker-compose up -d --scale httpd=3 Creating network &quot;httpd-service_default&quot; with the default driverCreating httpd-service_httpd_1 ... doneCreating httpd-service_httpd_2 ... doneCreating httpd-service_httpd_3 ... doneAnd we test web access (either by browser or command line):$ curl -IL http://binsh.ioHTTP/1.1 308 Permanent RedirectLocation: https://binsh.io/HTTP/2 200 $ curl -IL http://traefik.binsh.ioHTTP/1.1 308 Permanent RedirectLocation: https://traefik.binsh.io/HTTP/2 401 www-authenticate: Basic realm=&quot;traefik&quot;Both routes are working as expected (redirection from http to https and basic-auth on Traefik dashboard), so we can move on the next partAdd prometheus and grafana logic for Traefik routingLet’s add prometheus and grafana routing on Traefik configuration now !We don’t have much to do as we already did it for binsh.ioFirst we create two DNS records, we could use A (IP &amp;lt;-&amp;gt; FQDN) or CNAME (FQDN &amp;lt;-&amp;gt; FQDN) types here, but I will prefer using CNAME for now as they are hosted on the same IP (saves cost and time for your computer, you are welcome)Here we use nslookup to check DNS records, It’s packaged in bind-utils on Centos8 (not installed by default)$ nslookup prometheus.binsh.ioNon-authoritative answer:prometheus.binsh.io canonical name = binsh.io.Name: binsh.io$ nslookup grafana.binsh.io Non-authoritative answer:grafana.binsh.io canonical name = binsh.io.Name: binsh.ioAnd we get the resulting dynamic configuration for Traefik:$ cat dynamic_conf.yml http: routers: http_router: rule: &quot;Host(`binsh.io`)&quot; service: web middlewares: - traefik-https-redirect tls: certResolver: letsencrypt grafana_router: rule: &quot;Host(`grafana.binsh.io`)&quot; service: grafana middlewares: - traefik-https-redirect - traefik-auth tls: certResolver: letsencrypt prometheus_router: rule: &quot;Host(`prometheus.binsh.io`)&quot; service: prometheus middlewares: - traefik-https-redirect - traefik-auth tls: certResolver: letsencrypt traefik_router: entrypoints: http rule: &quot;Host(`traefik.binsh.io`)&quot; service: api@internal middlewares: - traefik-https-redirect traefik_secure_router: entrypoints: https rule: &quot;Host(`traefik.binsh.io`)&quot; middlewares: - traefik-auth tls: certResolver: letsencrypt service: api@internal services: web: loadBalancer: servers: - url: &quot;http://httpd/&quot; grafana: loadBalancer: servers: - url: &quot;http://grafana:3000/&quot; prometheus: loadBalancer: servers: - url: &quot;http://prometheus:9090/&quot; middlewares: traefik-auth: basicAuth: users: - &quot;&amp;lt;USER&amp;gt;:$apr1$&amp;lt;PASSWORD VERY SECURE&amp;gt;&quot; traefik-https-redirect: redirectScheme: scheme: https permanent: trueWe added two routers and services, one for Grafana on port 3000 and one for Prometheus on port 9090. Unfortunately Traefik is unable to find these port dynamically like he did for httpd. I guess we would have to change prom/grafana default port to 80 and It could work without indicating ports, but why bother.We also added HTTPS redirection and basic-auth on both services, because why not.Now we will be able to access prometheus and grafana through Traefik as long as they are on the same docker network.Put prometheus and grafana behind TraefikNow we can move Grafana and Prometheus into their own network space:The resulting docker-compose file:$ cat docker-compose.yml version: &quot;3.9&quot;services: prometheus: image: &quot;prom/prometheus&quot; volumes: - prometheus-storage:/prometheus - $PWD/prometheus.yml:/etc/prometheus/prometheus.yml networks: - traefik grafana: image: &quot;grafana/grafana-oss&quot; volumes: - grafana-storage:/var/lib/grafana - $PWD/grafana.ini:/etc/grafana/grafana.ini depends_on: - &quot;prometheus&quot; networks: - traefiknetworks: traefik: external: true name: traefik-service_defaultvolumes: grafana-storage: prometheus-storage:We can remove any port exposure/mapping, as they will connect to traefik network. We could have created an internal network for prometheus -&amp;gt; grafana traffic but It would not add much value.Now let’s try to start our prometheus/grafana services and test them through TraefikCan we still access those services through our custom ports (36200/36300) ? No we can’t since we removed the exposure from the docker-compose file$ curl http://binsh.io:36200curl: (7) Failed to connect to binsh.io port 36200: Connection refused$ curl http://binsh.io:36300curl: (7) Failed to connect to binsh.io port 36300: Connection refusedCan we access them through Traefik ? Yes, and It even redirects to https with a valid TLS certificate and basic authentication, nice !We reached our goal, and even removed any left-over ports that were not useful, so we can update our architecture as such:Node_exporter and CadvisorNow that we have somewhere to send our monitoring onto, we can start collecting informations across the boardWe will node_exporter for collecting host metric (how is our hardware doing) and Cadvisor for docker metrics (are our containers running into any bottlenecks)https://hub.docker.com/r/prom/node-exporterhttps://hub.docker.com/r/google/cadvisor/We create a new directory where we can begin our docker-compose file$ cat agent-monitoring-service$ touch docker-compose.yml** UNDER CONSTRUCTION **" } ]
